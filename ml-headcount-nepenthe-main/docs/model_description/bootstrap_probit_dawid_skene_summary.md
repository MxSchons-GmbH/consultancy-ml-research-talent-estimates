# Bootstrap Probit Dawid-Skene Algorithm Summary

The algorithm estimates ML expert headcount in company populations from noisy crowdsourced annotations of sampled employees using a **bootstrap probit Dawid-Skene model**. The core innovation extends the classical Dawid-Skene framework—which characterizes each annotator by a confusion matrix—to allow for **correlated errors** between annotators through a multivariate normal probit model. Rather than assuming annotators are conditionally independent given the true label, the model introduces latent continuous scores that follow a multivariate normal distribution with separate mean vectors and covariance matrices for each true label class. This captures situations where annotators share similar biases or make correlated mistakes.

For inference, the algorithm uses Bayes' theorem to compute the posterior probability that each employee is an ML expert, given their annotation pattern. The likelihoods are multivariate normal orthant probabilities—computed via Quasi-Monte Carlo integration—that properly account for annotator correlations. These individual probabilities are then used to sample binary expertise labels via Bernoulli draws, and the sampled labels are summed within each company to obtain realized ML headcounts. This Bernoulli sampling step is crucial for obtaining the full posterior predictive distribution rather than just the distribution of expected counts.

Uncertainty quantification is achieved through a **single-loop bootstrap** that simultaneously resamples four sources of variation: (1) validation data to capture parameter estimation uncertainty, (2) the class prior from a Beta(2,10) hyperprior to reflect uncertainty about the base rate of ML experts, (3) test data within companies to capture sampling uncertainty in which employees are observed, and (4) latent expertise labels via Bernoulli sampling to capture realization uncertainty. Each bootstrap iteration re-estimates confusion matrices and covariance matrices from the resampled validation data, samples a new class prior, resamples employees within each company, and then samples binary expertise labels for each employee before aggregating. The resulting bootstrap distribution provides both point estimates (means) and credible intervals (percentiles) for company-level headcounts, properly accounting for all sources of uncertainty in the annotation, sampling, and aggregation process.
