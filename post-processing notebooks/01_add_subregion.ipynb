{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Nepenthe Processing Pipeline\n\nThis notebook processes organization data through the following steps:\n\n1. **Add Subregion**: Extract country from headquarters location and map to UN M49 subregions\n2. **Create Subgroups**: Filter organizations into different ML consultancy categories\n3. **Summary Table**: Generate comparison table across all subgroups\n\n**Input:** `final_results_main_orgs - final_results_main_orgs.csv`  \n**Output:** Comparison table with organization counts, employee totals, ML estimates, and regional breakdowns"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle, Patch\nimport matplotlib.ticker as mticker\nimport warnings\nimport re\nfrom matplotlib.lines import Line2D\nfrom matplotlib.patches import FancyBboxPatch\nfrom matplotlib.colors import LinearSegmentedColormap\nwarnings.filterwarnings('ignore')\n\n# =============================================================================\n# DESIGN SYSTEM \u2014 Academic/Scientific Style\n# =============================================================================\n# Inspired by Nature/Science journals: clean, sophisticated, publication-ready\n\n# Typography Configuration\nFONT_FAMILY = 'Helvetica Neue'  # Falls back to Helvetica, then Arial\nFONT_SIZES = {\n    'title': 14,\n    'subtitle': 11,\n    'axis_label': 11,\n    'tick_label': 9,\n    'legend': 9,\n    'annotation': 8,\n    'org_label': 7,\n}\n\n# Set matplotlib defaults\nplt.rcParams.update({\n    'font.family': 'sans-serif',\n    'font.sans-serif': ['Helvetica Neue', 'Helvetica', 'Arial', 'DejaVu Sans'],\n    'font.size': FONT_SIZES['tick_label'],\n    'axes.titlesize': FONT_SIZES['title'],\n    'axes.labelsize': FONT_SIZES['axis_label'],\n    'xtick.labelsize': FONT_SIZES['tick_label'],\n    'ytick.labelsize': FONT_SIZES['tick_label'],\n    'legend.fontsize': FONT_SIZES['legend'],\n    'figure.titlesize': FONT_SIZES['title'],\n    'axes.titleweight': 'medium',\n    'axes.labelweight': 'regular',\n    'axes.linewidth': 0.8,\n    'axes.spines.top': False,\n    'axes.spines.right': False,\n    'axes.grid': True,\n    'grid.alpha': 0.25,\n    'grid.linewidth': 0.5,\n    'grid.linestyle': '-',\n    'figure.facecolor': 'white',\n    'axes.facecolor': 'white',\n    'savefig.facecolor': 'white',\n    'savefig.dpi': 300,\n})\n\n# =============================================================================\n# COLOR PALETTE \u2014 Refined Academic\n# =============================================================================\n# Muted, sophisticated colors with good contrast and colorblind accessibility\n\n# Primary palette for categorical data (6 colors)\nPALETTE = {\n    'blue':     '#3C5488',  # Deep slate blue\n    'red':      '#DC3220',  # Vermillion (colorblind-safe red)\n    'green':    '#009988',  # Teal/cyan-green\n    'gold':     '#E68613',  # Amber/ochre  \n    'purple':   '#7B4B94',  # Muted violet\n    'gray':     '#868686',  # Neutral gray\n}\n\n# Sequential list for scatter plots\nPALETTE_LIST = [\n    PALETTE['blue'],\n    PALETTE['red'], \n    PALETTE['green'],\n    PALETTE['gold'],\n    PALETTE['purple'],\n    PALETTE['gray'],\n]\n\n# Specific role colors\nCOLORS = {\n    # Confidence intervals\n    'ci_pure_probit': '#2C6E49',      # Forest green\n    'ci_adjusted_synthetic': '#7B4B94', # Purple\n    \n    # Confidence category colors (landscape plot)\n    'probable':     '#2C6E49',   # Forest green (CI excludes zero)\n    'possible':     '#E68613',   # Amber (central estimate positive)\n    'nonzero':      '#7EB5D6',   # Light blue (upper bound positive)\n    'not_detected': '#868686',   # Gray (all zeros)\n    \n    # Geographic/thematic\n    'primary':    '#3C5488',   # Main brand blue\n    'secondary':  '#009988',   # Accent teal\n    'muted':      '#868686',   # Muted elements\n    'background': '#F5F5F5',   # Light background\n    'gridline':   '#E0E0E0',   # Grid color\n}\n\n# Landscape plot configuration\nLANDSCAPE_PALETTE = {\n    \"Probable\":     COLORS['probable'],\n    \"Possible\":     COLORS['possible'],\n    \"Non-zero\":     COLORS['nonzero'],\n    \"Not Detected\": COLORS['not_detected'],\n}\n\nLANDSCAPE_MARKERS = {\n    \"Probable\":     \"D\",   # Diamond\n    \"Possible\":     \"s\",   # Square\n    \"Non-zero\":     \"^\",   # Triangle up\n    \"Not Detected\": \"o\",   # Circle\n}\n\n# Estimator colors and markers (for Step 4 plot)\nESTIMATOR_STYLES = {\n    # Keyword filters \u2014 circles with muted tones\n    'filter_broad_yes':           {'color': PALETTE['gray'],   'marker': 'o', 'size': 24},\n    'filter_strict_no':           {'color': PALETTE['gray'],   'marker': 'v', 'size': 24},\n    'filter_broad_yes_strict_no': {'color': '#5A5A5A',         'marker': 's', 'size': 24},\n    # LLM estimates \u2014 distinctive colors, larger\n    'claude_total_accepted':      {'color': PALETTE['blue'],   'marker': 'D', 'size': 36},\n    'gpt5_total_accepted':        {'color': PALETTE['green'],  'marker': '^', 'size': 36},\n    'gemini_total_accepted':      {'color': PALETTE['gold'],   'marker': 'P', 'size': 40},\n}\n\n# Estimator labels (Step 4)\nESTIMATOR_LABELS = {\n    'filter_broad_yes': 'Keyword: Broad Yes',\n    'filter_strict_no': 'Keyword: Strict No', \n    'filter_broad_yes_strict_no': 'Keyword: Broad+Strict',\n    'claude_total_accepted': 'Claude (sonnet-4)',\n    'gpt5_total_accepted': 'GPT-5-mini',\n    'gemini_total_accepted': 'Gemini 2.5 Flash',\n}\n\n# =============================================================================\n# CONFIGURATION\n# =============================================================================\n\nSAVE_OUTPUTS = True  # Set to True to save output CSVs\nDATA_DIR = Path('..') if Path('../2026-01-28_final_results_main_orgs.csv').exists() else Path('.')\n\ndef assign_confidence_category(q10, q50, q90):\n    \"\"\"\n    Assign organization to confidence category based on statistical estimates.\n    \n    Categories (mutually exclusive, checked in order):\n    - Probable: q10 > 0 (80% CI excludes zero)\n    - Possible: q50 > 0, q10 = 0 (central estimate positive but uncertain)\n    - Non-zero: q90 > 0, q50 = 0 (upper bound positive only)\n    - Not Detected: all zeros (no ML signal)\n    \"\"\"\n    if pd.isna(q10) or pd.isna(q50) or pd.isna(q90):\n        return \"Not Detected\"\n    if q10 > 0:\n        return \"Probable\"\n    if q50 > 0:\n        return \"Possible\"\n    if q90 > 0:\n        return \"Non-zero\"\n    return \"Not Detected\"\n\n\n# =============================================================================\n# HELPER FUNCTIONS \u2014 Figure Styling\n# =============================================================================\n\ndef format_log_axis(ax, axis='y', limits=(1, 10000)):\n    \"\"\"Format log-scale axis with clean tick labels.\"\"\"\n    if axis == 'y':\n        ax.set_yscale('log')\n        ax.set_ylim(limits)\n        # Clean integer ticks\n        ticks = [t for t in [1, 10, 100, 1000, 10000] if limits[0] <= t <= limits[1]]\n        ax.set_yticks(ticks)\n        ax.set_yticklabels([f'{t:,}' if t >= 1000 else str(t) for t in ticks])\n        ax.yaxis.set_minor_locator(mticker.NullLocator())\n    else:\n        ax.set_xscale('log')\n        ax.set_xlim(limits)\n        ticks = [t for t in [1, 10, 100, 1000, 10000] if limits[0] <= t <= limits[1]]\n        ax.set_xticks(ticks)\n        ax.set_xticklabels([f'{t:,}' if t >= 1000 else str(t) for t in ticks])\n        ax.xaxis.set_minor_locator(mticker.NullLocator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Add Subregion Column\n",
    "\n",
    "Extract country information from headquarters location strings and map to UN M49 subregions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country name variations mapping\n",
    "COUNTRY_VARIATIONS = {\n",
    "    'USA': 'United States',\n",
    "    'US': 'United States',\n",
    "    'UK': 'United Kingdom',\n",
    "    'United Kingdom of Great Britain and Northern Ireland': 'United Kingdom',\n",
    "    'The Netherlands': 'Netherlands',\n",
    "    'NA - South Africa': 'South Africa',\n",
    "    'NA - Vietnam': 'Vietnam',\n",
    "    'NA - Uruguay': 'Uruguay',\n",
    "    'Russian Federation': 'Russia',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UN M49 subregion mapping\n",
    "COUNTRY_TO_SUBREGION = {\n",
    "    # Northern America\n",
    "    'United States': 'Northern America',\n",
    "    'Canada': 'Northern America',\n",
    "    'Bermuda': 'Northern America',\n",
    "    'Greenland': 'Northern America',\n",
    "    'Saint Pierre and Miquelon': 'Northern America',\n",
    "\n",
    "    # Central America\n",
    "    'Belize': 'Central America',\n",
    "    'Costa Rica': 'Central America',\n",
    "    'El Salvador': 'Central America',\n",
    "    'Guatemala': 'Central America',\n",
    "    'Honduras': 'Central America',\n",
    "    'Mexico': 'Central America',\n",
    "    'Nicaragua': 'Central America',\n",
    "    'Panama': 'Central America',\n",
    "\n",
    "    # South America\n",
    "    'Argentina': 'South America',\n",
    "    'Bolivia': 'South America',\n",
    "    'Brazil': 'South America',\n",
    "    'Chile': 'South America',\n",
    "    'Colombia': 'South America',\n",
    "    'Ecuador': 'South America',\n",
    "    'French Guiana': 'South America',\n",
    "    'Guyana': 'South America',\n",
    "    'Paraguay': 'South America',\n",
    "    'Peru': 'South America',\n",
    "    'Suriname': 'South America',\n",
    "    'Uruguay': 'South America',\n",
    "    'Venezuela': 'South America',\n",
    "\n",
    "    # Western Europe\n",
    "    'Austria': 'Western Europe',\n",
    "    'Belgium': 'Western Europe',\n",
    "    'France': 'Western Europe',\n",
    "    'Germany': 'Western Europe',\n",
    "    'Liechtenstein': 'Western Europe',\n",
    "    'Luxembourg': 'Western Europe',\n",
    "    'Monaco': 'Western Europe',\n",
    "    'Netherlands': 'Western Europe',\n",
    "    'Switzerland': 'Western Europe',\n",
    "\n",
    "    # Northern Europe\n",
    "    'Denmark': 'Northern Europe',\n",
    "    'Estonia': 'Northern Europe',\n",
    "    'Finland': 'Northern Europe',\n",
    "    'Iceland': 'Northern Europe',\n",
    "    'Ireland': 'Northern Europe',\n",
    "    'Latvia': 'Northern Europe',\n",
    "    'Lithuania': 'Northern Europe',\n",
    "    'Norway': 'Northern Europe',\n",
    "    'Sweden': 'Northern Europe',\n",
    "    'United Kingdom': 'Northern Europe',\n",
    "\n",
    "    # Eastern Europe\n",
    "    'Belarus': 'Eastern Europe',\n",
    "    'Bulgaria': 'Eastern Europe',\n",
    "    'Czech Republic': 'Eastern Europe',\n",
    "    'Czechia': 'Eastern Europe',\n",
    "    'Hungary': 'Eastern Europe',\n",
    "    'Poland': 'Eastern Europe',\n",
    "    'Moldova': 'Eastern Europe',\n",
    "    'Romania': 'Eastern Europe',\n",
    "    'Russia': 'Eastern Europe',\n",
    "    'Russian Federation': 'Eastern Europe',\n",
    "    'Slovakia': 'Eastern Europe',\n",
    "    'Ukraine': 'Eastern Europe',\n",
    "\n",
    "    # Southern Europe\n",
    "    'Albania': 'Southern Europe',\n",
    "    'Andorra': 'Southern Europe',\n",
    "    'Bosnia and Herzegovina': 'Southern Europe',\n",
    "    'Croatia': 'Southern Europe',\n",
    "    'Gibraltar': 'Southern Europe',\n",
    "    'Greece': 'Southern Europe',\n",
    "    'Italy': 'Southern Europe',\n",
    "    'Malta': 'Southern Europe',\n",
    "    'Montenegro': 'Southern Europe',\n",
    "    'North Macedonia': 'Southern Europe',\n",
    "    'Portugal': 'Southern Europe',\n",
    "    'San Marino': 'Southern Europe',\n",
    "    'Serbia': 'Southern Europe',\n",
    "    'Slovenia': 'Southern Europe',\n",
    "    'Spain': 'Southern Europe',\n",
    "    'Vatican City': 'Southern Europe',\n",
    "\n",
    "    # Eastern Asia\n",
    "    'China': 'Eastern Asia',\n",
    "    'Hong Kong': 'Eastern Asia',\n",
    "    'Japan': 'Eastern Asia',\n",
    "    'Macao': 'Eastern Asia',\n",
    "    'Mongolia': 'Eastern Asia',\n",
    "    'North Korea': 'Eastern Asia',\n",
    "    'South Korea': 'Eastern Asia',\n",
    "    'Taiwan': 'Eastern Asia',\n",
    "\n",
    "    # South-Eastern Asia\n",
    "    'Brunei': 'South-Eastern Asia',\n",
    "    'Cambodia': 'South-Eastern Asia',\n",
    "    'Indonesia': 'South-Eastern Asia',\n",
    "    'Laos': 'South-Eastern Asia',\n",
    "    'Malaysia': 'South-Eastern Asia',\n",
    "    'Myanmar': 'South-Eastern Asia',\n",
    "    'Philippines': 'South-Eastern Asia',\n",
    "    'Singapore': 'South-Eastern Asia',\n",
    "    'Thailand': 'South-Eastern Asia',\n",
    "    'Timor-Leste': 'South-Eastern Asia',\n",
    "    'Vietnam': 'South-Eastern Asia',\n",
    "\n",
    "    # Southern Asia\n",
    "    'Afghanistan': 'Southern Asia',\n",
    "    'Bangladesh': 'Southern Asia',\n",
    "    'Bhutan': 'Southern Asia',\n",
    "    'India': 'Southern Asia',\n",
    "    'Iran': 'Southern Asia',\n",
    "    'Maldives': 'Southern Asia',\n",
    "    'Nepal': 'Southern Asia',\n",
    "    'Pakistan': 'Southern Asia',\n",
    "    'Sri Lanka': 'Southern Asia',\n",
    "\n",
    "    # Western Asia\n",
    "    'Armenia': 'Western Asia',\n",
    "    'Azerbaijan': 'Western Asia',\n",
    "    'Bahrain': 'Western Asia',\n",
    "    'Cyprus': 'Western Asia',\n",
    "    'Georgia': 'Western Asia',\n",
    "    'Iraq': 'Western Asia',\n",
    "    'Israel': 'Western Asia',\n",
    "    'Jordan': 'Western Asia',\n",
    "    'Kuwait': 'Western Asia',\n",
    "    'Lebanon': 'Western Asia',\n",
    "    'Oman': 'Western Asia',\n",
    "    'Qatar': 'Western Asia',\n",
    "    'Saudi Arabia': 'Western Asia',\n",
    "    'Syria': 'Western Asia',\n",
    "    'Turkey': 'Western Asia',\n",
    "    'United Arab Emirates': 'Western Asia',\n",
    "    'Yemen': 'Western Asia',\n",
    "\n",
    "    # Central Asia\n",
    "    'Kazakhstan': 'Central Asia',\n",
    "    'Kyrgyzstan': 'Central Asia',\n",
    "    'Tajikistan': 'Central Asia',\n",
    "    'Turkmenistan': 'Central Asia',\n",
    "    'Uzbekistan': 'Central Asia',\n",
    "\n",
    "    # Northern Africa\n",
    "    'Algeria': 'Northern Africa',\n",
    "    'Egypt': 'Northern Africa',\n",
    "    'Libya': 'Northern Africa',\n",
    "    'Morocco': 'Northern Africa',\n",
    "    'Sudan': 'Northern Africa',\n",
    "    'Tunisia': 'Northern Africa',\n",
    "    'Western Sahara': 'Northern Africa',\n",
    "\n",
    "    # Eastern Africa\n",
    "    'Burundi': 'Eastern Africa',\n",
    "    'Comoros': 'Eastern Africa',\n",
    "    'Djibouti': 'Eastern Africa',\n",
    "    'Eritrea': 'Eastern Africa',\n",
    "    'Ethiopia': 'Eastern Africa',\n",
    "    'Kenya': 'Eastern Africa',\n",
    "    'Madagascar': 'Eastern Africa',\n",
    "    'Malawi': 'Eastern Africa',\n",
    "    'Mauritius': 'Eastern Africa',\n",
    "    'Mozambique': 'Eastern Africa',\n",
    "    'Rwanda': 'Eastern Africa',\n",
    "    'Seychelles': 'Eastern Africa',\n",
    "    'Somalia': 'Eastern Africa',\n",
    "    'South Sudan': 'Eastern Africa',\n",
    "    'Tanzania': 'Eastern Africa',\n",
    "    'Uganda': 'Eastern Africa',\n",
    "    'Zambia': 'Eastern Africa',\n",
    "    'Zimbabwe': 'Eastern Africa',\n",
    "\n",
    "    # Southern Africa\n",
    "    'Botswana': 'Southern Africa',\n",
    "    'Eswatini': 'Southern Africa',\n",
    "    'Lesotho': 'Southern Africa',\n",
    "    'Namibia': 'Southern Africa',\n",
    "    'South Africa': 'Southern Africa',\n",
    "\n",
    "    # Western Africa\n",
    "    'Benin': 'Western Africa',\n",
    "    'Burkina Faso': 'Western Africa',\n",
    "    'Cabo Verde': 'Western Africa',\n",
    "    \"C\u00f4te d'Ivoire\": 'Western Africa',\n",
    "    'Gambia': 'Western Africa',\n",
    "    'Ghana': 'Western Africa',\n",
    "    'Guinea': 'Western Africa',\n",
    "    'Guinea-Bissau': 'Western Africa',\n",
    "    'Liberia': 'Western Africa',\n",
    "    'Mali': 'Western Africa',\n",
    "    'Mauritania': 'Western Africa',\n",
    "    'Niger': 'Western Africa',\n",
    "    'Nigeria': 'Western Africa',\n",
    "    'Senegal': 'Western Africa',\n",
    "    'Sierra Leone': 'Western Africa',\n",
    "    'Togo': 'Western Africa',\n",
    "\n",
    "    # Middle Africa\n",
    "    'Angola': 'Middle Africa',\n",
    "    'Cameroon': 'Middle Africa',\n",
    "    'Central African Republic': 'Middle Africa',\n",
    "    'Chad': 'Middle Africa',\n",
    "    'Congo': 'Middle Africa',\n",
    "    'Democratic Republic of the Congo': 'Middle Africa',\n",
    "    'Equatorial Guinea': 'Middle Africa',\n",
    "    'Gabon': 'Middle Africa',\n",
    "    'S\u00e3o Tom\u00e9 and Pr\u00edncipe': 'Middle Africa',\n",
    "\n",
    "    # Australia and New Zealand\n",
    "    'Australia': 'Australia and New Zealand',\n",
    "    'New Zealand': 'Australia and New Zealand',\n",
    "\n",
    "    # Caribbean\n",
    "    'Antigua and Barbuda': 'Caribbean',\n",
    "    'Bahamas': 'Caribbean',\n",
    "    'Barbados': 'Caribbean',\n",
    "    'Cuba': 'Caribbean',\n",
    "    'Dominica': 'Caribbean',\n",
    "    'Dominican Republic': 'Caribbean',\n",
    "    'Grenada': 'Caribbean',\n",
    "    'Haiti': 'Caribbean',\n",
    "    'Jamaica': 'Caribbean',\n",
    "    'Saint Kitts and Nevis': 'Caribbean',\n",
    "    'Saint Lucia': 'Caribbean',\n",
    "    'Saint Vincent and the Grenadines': 'Caribbean',\n",
    "    'Trinidad and Tobago': 'Caribbean',\n",
    "\n",
    "    # Melanesia\n",
    "    'Fiji': 'Melanesia',\n",
    "    'New Caledonia': 'Melanesia',\n",
    "    'Papua New Guinea': 'Melanesia',\n",
    "    'Solomon Islands': 'Melanesia',\n",
    "    'Vanuatu': 'Melanesia',\n",
    "\n",
    "    # Micronesia\n",
    "    'Guam': 'Micronesia',\n",
    "    'Kiribati': 'Micronesia',\n",
    "    'Marshall Islands': 'Micronesia',\n",
    "    'Micronesia': 'Micronesia',\n",
    "    'Nauru': 'Micronesia',\n",
    "    'Northern Mariana Islands': 'Micronesia',\n",
    "    'Palau': 'Micronesia',\n",
    "\n",
    "    # Polynesia\n",
    "    'American Samoa': 'Polynesia',\n",
    "    'Cook Islands': 'Polynesia',\n",
    "    'French Polynesia': 'Polynesia',\n",
    "    'Niue': 'Polynesia',\n",
    "    'Samoa': 'Polynesia',\n",
    "    'Tonga': 'Polynesia',\n",
    "    'Tuvalu': 'Polynesia',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_country_from_location(location):\n",
    "    \"\"\"Extract country from headquarters location string.\"\"\"\n",
    "    if pd.isna(location) or location == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    # Split by comma and take the last part as country\n",
    "    parts = [part.strip() for part in str(location).split(',')]\n",
    "    country = parts[-1] if parts else \"\"\n",
    "    \n",
    "    return COUNTRY_VARIATIONS.get(country, country)\n",
    "\n",
    "\n",
    "def map_country_to_subregion(country):\n",
    "    \"\"\"Map country to UN M49 subregion.\"\"\"\n",
    "    return COUNTRY_TO_SUBREGION.get(country, 'Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "input_file = DATA_DIR / '2026-01-28_final_results_main_orgs.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "print(f\"Loaded {len(df)} rows from {input_file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify required column exists\n",
    "if 'headquarters_location' not in df.columns:\n",
    "    raise ValueError(f\"'headquarters_location' column not found. Available: {list(df.columns)}\")\n",
    "\n",
    "# Extract countries and map to subregions\n",
    "df['Country'] = df['headquarters_location'].apply(extract_country_from_location)\n",
    "df['Subregion'] = df['Country'].apply(map_country_to_subregion)\n",
    "\n",
    "print(\"Added Country and Subregion columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "total_rows = len(df)\n",
    "unknown_count = len(df[df['Subregion'] == 'Unknown'])\n",
    "success_rate = (total_rows - unknown_count) / total_rows * 100\n",
    "\n",
    "print(f\"Subregion Mapping Summary\")\n",
    "print(f\"=\"*40)\n",
    "print(f\"Total locations processed: {total_rows}\")\n",
    "print(f\"Successfully mapped: {total_rows - unknown_count}\")\n",
    "print(f\"Unknown mappings: {unknown_count}\")\n",
    "print(f\"Success rate: {success_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subregion distribution\n",
    "print(\"Subregion Distribution\")\n",
    "print(\"=\"*40)\n",
    "subregion_counts = df['Subregion'].value_counts()\n",
    "for subregion, count in subregion_counts.items():\n",
    "    print(f\"  {subregion}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show unknown countries if any\n",
    "if unknown_count > 0:\n",
    "    print(\"Unknown countries found:\")\n",
    "    unknown_countries = df[df['Subregion'] == 'Unknown']['Country'].value_counts()\n",
    "    for country, count in unknown_countries.items():\n",
    "        print(f\"  {country}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview results\n",
    "print(\"Sample of processed data:\")\n",
    "df[['headquarters_location', 'Country', 'Subregion']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Create Subgroups\n",
    "\n",
    "Filter organizations into confidence categories based on ML estimate uncertainty:\n",
    "\n",
    "| Category | Criteria | Description |\n",
    "|----------|----------|-------------|\n",
    "| **All** | - | All 403 organizations |\n",
    "| **Probable** | q10 > 0 | 80% CI excludes zero - confident ML presence |\n",
    "| **Possible** | q50 > 0, q10 = 0 | Central estimate positive but CI includes zero |\n",
    "| **Non-zero** | q90 > 0, q50 = 0 | Upper bound positive but central estimate zero |\n",
    "| **Not Detected** | q90 = q50 = q10 = 0 | All estimates zero - no ML signal |\n",
    "\n",
    "Where q10/q50/q90 use pure probit estimates when available, otherwise adjusted synthetic estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define key columns\n",
    "ML_STAFF_COL = 'adjusted_synthetic_q50'\n",
    "HEADCOUNT_COL = 'total_headcount' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify required columns exist\n",
    "required_cols = [ML_STAFF_COL, HEADCOUNT_COL]\n",
    "missing_cols = [c for c in required_cols if c not in df.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"Warning: Missing columns: {missing_cols}\")\n",
    "else:\n",
    "    print(\"All required columns present\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate ML estimate using fallback: q50 if available, else adjusted_synthetic_q50\nq50_pure = pd.to_numeric(df['q50'], errors='coerce') if 'q50' in df.columns else pd.Series(np.nan, index=df.index)\nq50_synthetic = pd.to_numeric(df['adjusted_synthetic_q50'], errors='coerce')\n\n# Use pure probit q50 when not empty, else synthetic\ndf['ml_estimate'] = q50_pure.where(q50_pure.notna(), q50_synthetic)\n\n# Calculate ML share using the combined estimate\nheadcount = pd.to_numeric(df[HEADCOUNT_COL], errors='coerce')\ndf['ml_share_calc'] = df['ml_estimate'] / headcount\n\n# Get values for masking\nml_count = df['ml_estimate']\nml_share = df['ml_share_calc']\n\n# Summary stats\nn_pure = q50_pure.notna().sum()\nn_synthetic = len(df) - n_pure\nprint(f\"ML estimate source: {n_pure} using pure probit q50, {n_synthetic} using adjusted synthetic q50\")\nprint(f\"ML staff (estimate) - min: {ml_count.min():.1f}, max: {ml_count.max():.1f}, median: {ml_count.median():.1f}\")\nprint(f\"ML share - min: {ml_share.min():.4f}, max: {ml_share.max():.4f}, median: {ml_share.median():.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subgroup masks\n",
    "\n",
    "# 1. All companies (no filter)\n",
    "mask_all = pd.Series([True] * len(df), index=df.index)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Confidence categories based on statistical estimates\n",
    "# Uses pure probit when available, otherwise adjusted synthetic\n",
    "# -----------------------------------------------------------------------------\n",
    "q10_pure = pd.to_numeric(df['q10'], errors='coerce')\n",
    "q50_pure = pd.to_numeric(df['q50'], errors='coerce')\n",
    "q90_pure = pd.to_numeric(df['q90'], errors='coerce')\n",
    "q10_synthetic = pd.to_numeric(df['adjusted_synthetic_q10'], errors='coerce')\n",
    "q50_synthetic = pd.to_numeric(df['adjusted_synthetic_q50'], errors='coerce')\n",
    "q90_synthetic = pd.to_numeric(df['adjusted_synthetic_q90'], errors='coerce')\n",
    "\n",
    "# Use pure probit when available (indicated by q50 not being NaN)\n",
    "effective_q10 = q10_pure.where(q50_pure.notna(), q10_synthetic)\n",
    "effective_q50 = q50_pure.where(q50_pure.notna(), q50_synthetic)\n",
    "effective_q90 = q90_pure.where(q50_pure.notna(), q90_synthetic)\n",
    "\n",
    "# 2. Probable: q10 > 0 (80% CI excludes zero - confident ML presence)\n",
    "mask_probable = (effective_q10 > 0)\n",
    "\n",
    "# 3. Possible: q50 > 0 but q10 = 0 (central estimate positive but uncertain)\n",
    "mask_possible = (effective_q50 > 0) & (effective_q10 == 0)\n",
    "\n",
    "# 4. Non-zero: q90 > 0 but q50 = 0 (upper bound positive only)\n",
    "mask_nonzero = (effective_q90 > 0) & (effective_q50 == 0)\n",
    "\n",
    "# 5. Not Detected: all zeros (no ML signal)\n",
    "mask_not_detected = (effective_q90 == 0) & (effective_q50 == 0) & (effective_q10 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filtered DataFrames\n",
    "df_all = df[mask_all].copy()\n",
    "df_probable = df[mask_probable].copy()\n",
    "df_possible = df[mask_possible].copy()\n",
    "df_nonzero = df[mask_nonzero].copy()\n",
    "df_not_detected = df[mask_not_detected].copy()\n",
    "\n",
    "# Store in dict for easy access\n",
    "subgroups = {\n",
    "    'all': df_all,\n",
    "    'probable': df_probable,\n",
    "    'possible': df_possible,\n",
    "    'nonzero': df_nonzero,\n",
    "    'not_detected': df_not_detected\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"Subgroup Summary\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Category':<20} {'Count':>8} {'Criteria'}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'All':<20} {len(df_all):>8} All organizations\")\n",
    "print(\"-\"*60)\n",
    "print(\"Confidence Categories (mutually exclusive):\")\n",
    "print(f\"{'Probable':<20} {len(df_probable):>8} q10 > 0 (CI excludes zero)\")\n",
    "print(f\"{'Possible':<20} {len(df_possible):>8} q50 > 0, q10 = 0\")\n",
    "print(f\"{'Non-zero':<20} {len(df_nonzero):>8} q90 > 0, q50 = 0\")\n",
    "print(f\"{'Not Detected':<20} {len(df_not_detected):>8} all zeros\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Categories total':<20} {len(df_probable) + len(df_possible) + len(df_nonzero) + len(df_not_detected):>8} (should equal All)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Preview each subgroup\npreview_cols = ['organization_name', HEADCOUNT_COL, 'ml_estimate', 'ml_share_calc', 'Subregion']\n\nfor name, subgroup_df in subgroups.items():\n    if len(subgroup_df) > 0:\n        print(f\"\\n{name.upper()} - Sample (first 5):\")\n        display(subgroup_df[preview_cols].head())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save outputs\n",
    "if SAVE_OUTPUTS:\n",
    "    output_dir = DATA_DIR / 'output'\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    output_files = {\n",
    "        'all': 'all_orgs.csv',\n",
    "        'probable': 'orgs_probable.csv',\n",
    "        'possible': 'orgs_possible.csv',\n",
    "        'nonzero': 'orgs_nonzero.csv',\n",
    "        'not_detected': 'orgs_not_detected.csv'\n",
    "    }\n",
    "    \n",
    "    for name, filename in output_files.items():\n",
    "        filepath = output_dir / filename\n",
    "        subgroups[name].to_csv(filepath, index=False)\n",
    "        print(f\"Saved {name}: {filepath} (N={len(subgroups[name])})\")\n",
    "else:\n",
    "    print(\"Output save skipped (set SAVE_OUTPUTS=True to enable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Step 3: Summary Comparison Table\n\nGenerate a comprehensive comparison table across all subgroups with:\n- Organization counts and percentages\n- Employee totals and medians\n- ML engineer estimates (q10, q50, q90)\n- ML talent percentages\n- Size breakdowns\n- Regional distributions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Summary table configuration\nPCT_DECIMALS = 1\nINDENT = \"\\u00A0\" * 4  # Non-breaking spaces for indentation\n\n# Regions in display order\nREGION_ORDER = [\n    \"Northern America\",\n    \"Western Europe\", \n    \"Southern Asia\",\n    \"Eastern Asia\",\n    \"Northern Europe\",\n    \"Eastern Europe\",\n    \"Western Asia\",\n    \"South America\",\n    \"Southern Europe\",\n    \"Unknown\",\n    \"Australia and New Zealand\",\n    \"South-Eastern Asia\",\n    \"Central America\",\n    \"Southern Africa\",\n    \"Northern Africa\",\n    \"Western Africa\",\n    \"Caribbean\",\n]\n\n# Size sections\nSIZE_SECTIONS = [\n    (\"Small (< 100 employees)\", lambda hc: hc < 100),\n    (\"Medium (100-999 employees)\", lambda hc: (hc >= 100) & (hc <= 999)),\n    (\"Large (1,000-9,999 employees)\", lambda hc: (hc >= 1000) & (hc <= 9999)),\n    (\"Giant (\u226510,000 employees)\", lambda hc: hc >= 10000),\n]",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Helper functions for formatting\n\ndef format_int_iso(n):\n    \"\"\"Format integers with spaces between groups of three digits.\"\"\"\n    if n is None or (isinstance(n, float) and not np.isfinite(n)) or pd.isna(n):\n        return \"\"\n    n = int(n)\n    sign = \"-\" if n < 0 else \"\"\n    s = str(abs(n))\n    groups = []\n    while s:\n        groups.append(s[-3:])\n        s = s[:-3]\n    return sign + \" \".join(reversed(groups))\n\ndef pct_string(numer, denom, decimals=PCT_DECIMALS):\n    \"\"\"Format as percentage string.\"\"\"\n    if denom is None or denom == 0 or not np.isfinite(denom) or pd.isna(denom):\n        return \"n/a\"\n    pct = 100.0 * float(numer) / float(denom)\n    return f\"{pct:.{decimals}f}%\"\n\ndef extract_year(series):\n    \"\"\"Extract year from 'Founded Date' column.\"\"\"\n    dt = pd.to_datetime(series, errors=\"coerce\")\n    years = dt.dt.year\n    years = years.where((years >= 1700) & (years <= 2100))\n    return years.astype(\"Int64\")\n\ndef weighted_mean(values, weights):\n    \"\"\"Headcount-weighted mean, ignoring NaNs.\"\"\"\n    mask = values.notna() & weights.notna()\n    if not mask.any():\n        return None\n    v = values[mask].astype(float)\n    w = weights[mask].astype(float)\n    wsum = float(w.sum())\n    if wsum <= 0:\n        return None\n    return float((v * w).sum()) / wsum",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def compute_section_metrics(df_sub, total_n, include_total_employees=False, include_median_year=False):\n    \"\"\"\n    Compute metrics for a section (total or size band).\n    Returns dict of row_label -> formatted string.\n    \"\"\"\n    out = {}\n    org_n = len(df_sub)\n    \n    # Organization N with percentage\n    out[\"Organization N\"] = f\"{format_int_iso(org_n)} ({pct_string(org_n, total_n)})\"\n    \n    # Total employees (only for Total section)\n    hc = pd.to_numeric(df_sub['total_headcount'], errors='coerce')\n    total_emp = int(hc.dropna().sum()) if hc.notna().any() else 0\n    \n    if include_total_employees:\n        out[\"Total employees\"] = format_int_iso(total_emp)\n    \n    # Median founding year (only for Total section)\n    if include_median_year and 'Founded Date' in df_sub.columns:\n        years = extract_year(df_sub['Founded Date'])\n        med_year = int(np.round(years.dropna().median())) if years.notna().any() else None\n        out[\"Median founding year\"] = \"\" if med_year is None else str(med_year)\n    \n    # Median total employees\n    hc_valid = hc.dropna()\n    med_emp = int(np.round(hc_valid.median())) if len(hc_valid) > 0 else None\n    out[\"Median total employees\"] = \"\" if med_emp is None else format_int_iso(med_emp)\n    \n    # ML engineers: sum of q50 with q10-q90 interval\n    ml_q50 = pd.to_numeric(df_sub['adjusted_synthetic_q50'], errors='coerce')\n    ml_q10 = pd.to_numeric(df_sub['adjusted_synthetic_q10'], errors='coerce')\n    ml_q90 = pd.to_numeric(df_sub['adjusted_synthetic_q90'], errors='coerce')\n    \n    total_q50 = int(ml_q50.dropna().sum()) if ml_q50.notna().any() else 0\n    total_q10 = int(ml_q10.dropna().sum()) if ml_q10.notna().any() else 0\n    total_q90 = int(ml_q90.dropna().sum()) if ml_q90.notna().any() else 0\n    \n    out[\"ML engineers (q50)\"] = f\"{format_int_iso(total_q50)} ({format_int_iso(total_q10)} - {format_int_iso(total_q90)})\"\n    \n    # Percentage ML talent (weighted by headcount)\n    if total_emp > 0:\n        base_pct = pct_string(total_q50, total_emp)\n        \n        # Calculate weighted mean share for interval\n        share_q10 = ml_q10 / hc\n        share_q90 = ml_q90 / hc\n        \n        low = weighted_mean(share_q10, hc)\n        high = weighted_mean(share_q90, hc)\n        \n        if low is not None and high is not None:\n            interval = f\"{100*low:.{PCT_DECIMALS}f}% - {100*high:.{PCT_DECIMALS}f}%\"\n            out[\"ML % of total\"] = f\"{base_pct} ({interval})\"\n        else:\n            out[\"ML % of total\"] = base_pct\n    else:\n        out[\"ML % of total\"] = \"n/a\"\n    \n    return out",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def compute_regional_breakdown(df_sub, total_n):\n    \"\"\"\n    Compute regional breakdown for orgs and employees.\n    Returns dict of row_label -> formatted string.\n    \"\"\"\n    out = {}\n    hc = pd.to_numeric(df_sub['total_headcount'], errors='coerce')\n    \n    for region in REGION_ORDER:\n        mask_r = (df_sub['Subregion'] == region)\n        \n        # Org count\n        count = int(mask_r.sum())\n        out[f\"{region} (orgs)\"] = f\"{format_int_iso(count)} ({pct_string(count, total_n)})\"\n        \n        # Employee sum  \n        emp_sum = int(hc[mask_r].dropna().sum()) if mask_r.any() else 0\n        out[f\"{region} (employees)\"] = format_int_iso(emp_sum)\n    \n    return out",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def summarize_subgroup(df_sub):\n    \"\"\"\n    Build complete summary for one subgroup.\n    Returns nested dict: section -> row_label -> value\n    \"\"\"\n    summary = {}\n    total_n = len(df_sub)\n    hc = pd.to_numeric(df_sub['total_headcount'], errors='coerce')\n    \n    # Total section\n    summary[\"Total\"] = compute_section_metrics(\n        df_sub, total_n, \n        include_total_employees=True, \n        include_median_year=True\n    )\n    \n    # Size sections\n    for size_label, cond in SIZE_SECTIONS:\n        mask = hc.notna() & cond(hc)\n        summary[size_label] = compute_section_metrics(\n            df_sub[mask], total_n,\n            include_total_employees=False,\n            include_median_year=False\n        )\n    \n    # Regional breakdown\n    summary[\"Regions\"] = compute_regional_breakdown(df_sub, total_n)\n    \n    return summary",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def build_comparison_table(subgroups_dict):\n",
    "    \"\"\"\n",
    "    Build comparison table across all subgroups.\n",
    "    Returns DataFrame with Characteristic as index and subgroups as columns.\n",
    "    \"\"\"\n",
    "    # Build row index structure: (display_name, row_type, key_for_lookup)\n",
    "    rows = []\n",
    "    \n",
    "    # Total section\n",
    "    rows.append((\"Total\", \"header\", None))\n",
    "    for lbl in [\"Organization N\", \"Total employees\", \"Median founding year\", \n",
    "                \"Median total employees\", \"ML engineers (q50)\", \"ML % of total\"]:\n",
    "        rows.append((f\"{INDENT}{lbl}\", \"row\", (\"Total\", lbl)))\n",
    "    \n",
    "    # Size sections\n",
    "    for size_label, _ in SIZE_SECTIONS:\n",
    "        rows.append((size_label, \"header\", None))\n",
    "        for lbl in [\"Organization N\", \"Median total employees\", \n",
    "                    \"ML engineers (q50)\", \"ML % of total\"]:\n",
    "            rows.append((f\"{INDENT}{lbl}\", \"row\", (size_label, lbl)))\n",
    "    \n",
    "    # Regions section - orgs first, then employees\n",
    "    rows.append((\"Regions (orgs)\", \"header\", None))\n",
    "    for r in REGION_ORDER:\n",
    "        # Display without \"(orgs)\", but lookup key still has it\n",
    "        rows.append((f\"{INDENT}{r}\", \"row\", (\"Regions\", f\"{r} (orgs)\")))\n",
    "    \n",
    "    rows.append((\"Regions (employees)\", \"header\", None))\n",
    "    for r in REGION_ORDER:\n",
    "        # Display without \"(employees)\", but lookup key still has it\n",
    "        rows.append((f\"{INDENT}{r}\", \"row\", (\"Regions\", f\"{r} (employees)\")))\n",
    "    \n",
    "    # Compute summaries for each subgroup\n",
    "    summaries = {name: summarize_subgroup(df) for name, df in subgroups_dict.items()}\n",
    "    \n",
    "    # Build data columns\n",
    "    data = {}\n",
    "    display_names = {\n",
    "        'all': 'All',\n",
    "        'probable': 'Probable',\n",
    "        'possible': 'Possible',\n",
    "        'nonzero': 'Non-zero',\n",
    "        'not_detected': 'Not Detected'\n",
    "    }\n",
    "    \n",
    "    for name, summ in summaries.items():\n",
    "        col_vals = []\n",
    "        for display, row_type, key in rows:\n",
    "            if row_type == \"header\":\n",
    "                col_vals.append(\"\")\n",
    "            else:\n",
    "                section, row_lbl = key\n",
    "                col_vals.append(summ.get(section, {}).get(row_lbl, \"\"))\n",
    "        data[display_names.get(name, name)] = col_vals\n",
    "    \n",
    "    # Create DataFrame\n",
    "    index_display = [r[0] for r in rows]\n",
    "    summary_df = pd.DataFrame(data, index=index_display)\n",
    "    summary_df.index.name = \"Characteristic\"\n",
    "    \n",
    "    return summary_df"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Build the comparison table\nsummary_table = build_comparison_table(subgroups)\nprint(f\"Summary table: {len(summary_table)} rows x {len(summary_table.columns)} columns\")\nsummary_table",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Optional: Save summary table\nif SAVE_OUTPUTS:\n    output_path = DATA_DIR / 'output' / 'combined_summary.csv'\n    summary_table.to_csv(output_path)\n    print(f\"Saved summary table to {output_path}\")\nelse:\n    print(\"Summary table save skipped (set SAVE_OUTPUTS=True to enable)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Step 3b: Individual Company Table\n\nDetailed table listing each organization with:\n- Company info (name, founded year, country, headcount)\n- Individual ML estimator values\n- Debiased log-median ML talent estimate with 80% CI\n- ML share percentage with 80% CI\n- Category classification (Enterprise/Mid-Scale/Boutique or \"-\")",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def create_company_table(df_input, subgroup_name=\"\"):\n",
    "    \"\"\"\n",
    "    Create detailed company table with ML estimates.\n",
    "    \n",
    "    Args:\n",
    "        df_input: DataFrame with organization data\n",
    "        subgroup_name: Name of the subgroup for display\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with formatted company data\n",
    "    \"\"\"\n",
    "    # Estimator columns in display order\n",
    "    estimator_cols = [\n",
    "        'filter_broad_yes_strict_no', \n",
    "        'filter_strict_no', \n",
    "        'filter_broad_yes',\n",
    "        'claude_total_accepted', \n",
    "        'gpt5_total_accepted', \n",
    "        'gemini_total_accepted'\n",
    "    ]\n",
    "    \n",
    "    rows = []\n",
    "    for _, row in df_input.iterrows():\n",
    "        # Extract founding year\n",
    "        founded_year = \"\"\n",
    "        if 'Founded Date' in row.index and pd.notna(row['Founded Date']):\n",
    "            try:\n",
    "                dt = pd.to_datetime(row['Founded Date'], errors='coerce')\n",
    "                if pd.notna(dt):\n",
    "                    founded_year = str(dt.year)\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "        \n",
    "        # Get individual estimator values (show 0 instead of -)\n",
    "        estimator_values = []\n",
    "        for col in estimator_cols:\n",
    "            if col in row.index:\n",
    "                val = pd.to_numeric(row[col], errors='coerce')\n",
    "                if pd.notna(val):\n",
    "                    estimator_values.append(str(int(val)))\n",
    "                else:\n",
    "                    estimator_values.append(\"-\")\n",
    "            else:\n",
    "                estimator_values.append(\"-\")\n",
    "        \n",
    "        # Determine if using synthetic estimate\n",
    "        q50_pure = pd.to_numeric(row.get('q50', np.nan), errors='coerce')\n",
    "        use_synthetic = pd.isna(q50_pure)\n",
    "        \n",
    "        # Get ML estimate (q50) and CI bounds\n",
    "        if use_synthetic:\n",
    "            ml_q50 = pd.to_numeric(row.get('adjusted_synthetic_q50', np.nan), errors='coerce')\n",
    "            ml_q10 = pd.to_numeric(row.get('adjusted_synthetic_q10', np.nan), errors='coerce')\n",
    "            ml_q90 = pd.to_numeric(row.get('adjusted_synthetic_q90', np.nan), errors='coerce')\n",
    "        else:\n",
    "            ml_q50 = q50_pure\n",
    "            ml_q10 = pd.to_numeric(row.get('q10', np.nan), errors='coerce')\n",
    "            ml_q90 = pd.to_numeric(row.get('q90', np.nan), errors='coerce')\n",
    "        \n",
    "        # Format ML talent estimate with CI\n",
    "        if pd.notna(ml_q50):\n",
    "            ml_str = f\"{format_int_iso(int(ml_q50))}\"\n",
    "            if pd.notna(ml_q10) and pd.notna(ml_q90):\n",
    "                ml_str += f\" ({format_int_iso(int(ml_q10))} - {format_int_iso(int(ml_q90))})\"\n",
    "            if use_synthetic:\n",
    "                ml_str += \" *\"  # Mark synthetic estimates\n",
    "        else:\n",
    "            ml_str = \"-\"\n",
    "        \n",
    "        # Get headcount and calculate ML share\n",
    "        headcount = pd.to_numeric(row.get('total_headcount', np.nan), errors='coerce')\n",
    "        \n",
    "        if pd.notna(ml_q50) and pd.notna(headcount) and headcount > 0:\n",
    "            ml_pct = 100.0 * ml_q50 / headcount\n",
    "            ml_pct_str = f\"{ml_pct:.2f}%\"\n",
    "            \n",
    "            if pd.notna(ml_q10) and pd.notna(ml_q90):\n",
    "                pct_low = 100.0 * ml_q10 / headcount\n",
    "                pct_high = 100.0 * ml_q90 / headcount\n",
    "                ml_pct_str += f\" ({pct_low:.2f}% - {pct_high:.2f}%)\"\n",
    "        else:\n",
    "            ml_pct_str = \"-\"\n",
    "        \n",
    "        # Determine confidence category using centralized function\n",
    "        category = assign_confidence_category(ml_q10, ml_q50, ml_q90)\n",
    "        \n",
    "        rows.append({\n",
    "            'Company Name': row.get('organization_name', ''),\n",
    "            'Founded': founded_year,\n",
    "            'Country': row.get('Country', ''),\n",
    "            'Total Staff (LinkedIn)': format_int_iso(int(headcount)) if pd.notna(headcount) else \"-\",\n",
    "            'Individual Estimates [broad+strict, strict, broad, claude, gpt5, gemini]': f\"[{', '.join(estimator_values)}]\",\n",
    "            'ML Talent q50 (q10 - q90)': ml_str,\n",
    "            'ML % of Total': ml_pct_str,\n",
    "            'Category': category\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Note for table display\n",
    "print(\"* = Synthetic estimate (adjusted_synthetic_q50 used when pure probit q50 is empty)\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create company table for Probable category (q10 > 0), sorted by ML talent estimate (descending)\n",
    "company_table = create_company_table(df_probable)\n",
    "\n",
    "# Sort by ML talent (need to extract numeric value for sorting)\n",
    "def extract_ml_for_sort(ml_str):\n",
    "    \"\"\"Extract numeric ML value for sorting.\"\"\"\n",
    "    if ml_str == \"-\":\n",
    "        return 0\n",
    "    # Extract first number (the q50 estimate)\n",
    "    match = re.match(r'([\\d\\s]+)', ml_str.replace(' ', ''))\n",
    "    if match:\n",
    "        return int(match.group(1).replace(' ', ''))\n",
    "    return 0\n",
    "\n",
    "company_table['_sort_key'] = company_table['ML Talent q50 (q10 - q90)'].apply(extract_ml_for_sort)\n",
    "company_table = company_table.sort_values('_sort_key', ascending=False).drop(columns=['_sort_key'])\n",
    "\n",
    "print(f\"Company Table \u2014 Probable ({len(company_table)} organizations)\")\n",
    "print(\"Sorted by ML Talent estimate (descending)\")\n",
    "print()\n",
    "company_table"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Optional: Save company table\n",
    "if SAVE_OUTPUTS:\n",
    "    output_path = DATA_DIR / 'output' / 'company_table_probable.csv'\n",
    "    company_table.to_csv(output_path, index=False)\n",
    "    print(f\"Saved company table to {output_path}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 3c: Individual Company Table \u2014 All Organizations\n\nSame detailed table as above, but for all 403 organizations in the dataset."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Create company table for ALL organizations, sorted by ML talent estimate (descending)\ncompany_table_all = create_company_table(df_all)\n\n# Sort by ML talent (reuse same sorting logic)\ncompany_table_all['_sort_key'] = company_table_all['ML Talent q50 (q10 - q90)'].apply(extract_ml_for_sort)\ncompany_table_all = company_table_all.sort_values('_sort_key', ascending=False).drop(columns=['_sort_key'])\n\nprint(f\"Company Table \u2014 All Organizations ({len(company_table_all)} organizations)\")\nprint(\"Sorted by ML Talent estimate (descending)\")\nprint()\ncompany_table_all"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Optional: Save company table for all organizations\nif SAVE_OUTPUTS:\n    output_path = DATA_DIR / 'output' / 'company_table_all.csv'\n    company_table_all.to_csv(output_path, index=False)\n    print(f\"Saved company table to {output_path}\")"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Step 4: ML Estimates Visualization\n",
    "\n",
    "Plot individual estimates and confidence intervals for organizations with Probable ML presence (q10 > 0).\n",
    "\n",
    "- Individual markers for each raw estimator (keyword filters + LLMs)\n",
    "- Confidence intervals: \n",
    "  - **Pure Probit 80% CI** (q10, q50, q90) when available\n",
    "  - **Adjusted Synthetic Probit 80% CI** when pure probit is empty"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def create_ml_estimates_plot(df_plot, figsize=(16, 8)):\n    \"\"\"\n    Create visualization of ML estimates for organizations.\n    \n    Design approach:\n    - Keyword filters shown as small, muted gray markers (background context)\n    - LLM estimates shown as distinct colored markers (primary focus)\n    - Confidence intervals with clear visual distinction (pure probit vs synthetic)\n    - Clean legend at bottom, outside plot area\n    \n    Args:\n        df_plot: DataFrame with organizations to plot\n        figsize: Figure size tuple\n    \n    Returns:\n        fig, ax, df_sorted\n    \"\"\"\n    # Estimator columns\n    filter_cols = ['filter_broad_yes', 'filter_strict_no', 'filter_broad_yes_strict_no']\n    llm_cols = ['gemini_total_accepted', 'claude_total_accepted', 'gpt5_total_accepted']\n    all_estimator_cols = [c for c in filter_cols + llm_cols if c in df_plot.columns]\n    \n    # Make a copy and prepare data\n    df_sorted = df_plot.copy()\n    \n    # Determine which CI to use for each org\n    df_sorted['_use_pure_probit'] = pd.to_numeric(df_sorted['q50'], errors='coerce').notna()\n    \n    # Get central estimate and bounds for each org\n    df_sorted['_central'] = np.where(\n        df_sorted['_use_pure_probit'],\n        pd.to_numeric(df_sorted['q50'], errors='coerce'),\n        pd.to_numeric(df_sorted['adjusted_synthetic_q50'], errors='coerce')\n    )\n    df_sorted['_lower'] = np.where(\n        df_sorted['_use_pure_probit'],\n        pd.to_numeric(df_sorted['q10'], errors='coerce'),\n        pd.to_numeric(df_sorted['adjusted_synthetic_q10'], errors='coerce')\n    )\n    df_sorted['_upper'] = np.where(\n        df_sorted['_use_pure_probit'],\n        pd.to_numeric(df_sorted['q90'], errors='coerce'),\n        pd.to_numeric(df_sorted['adjusted_synthetic_q90'], errors='coerce')\n    )\n    \n    # Sort by central estimate (consistent with create_ml_estimates_plot_all_orgs)\n    df_sorted['_sort_key'] = df_sorted['_central'].fillna(0)\n    df_sorted = df_sorted.sort_values('_sort_key').reset_index(drop=True)\n    \n    fig, ax = plt.subplots(figsize=figsize)\n    x = np.arange(len(df_sorted))\n    \n    # Offset for jittering points\n    offset_step = 0.10\n    \n    # -------------------------------------------------------------------------\n    # Layer 1: Keyword filter estimates (background, muted)\n    # -------------------------------------------------------------------------\n    filter_handles = []\n    for i, col in enumerate(filter_cols):\n        if col not in df_sorted.columns:\n            continue\n        y = pd.to_numeric(df_sorted[col], errors='coerce').values\n        mask = np.isfinite(y) & (y > 0)\n        x_pos = x + (i - 1) * offset_step\n        \n        style = ESTIMATOR_STYLES.get(col, {'color': PALETTE['gray'], 'marker': 'o', 'size': 24})\n        sc = ax.scatter(\n            x_pos[mask], y[mask],\n            s=style['size'], marker=style['marker'],\n            c=style['color'], alpha=0.35,\n            linewidths=0, zorder=1,\n            label=ESTIMATOR_LABELS.get(col, col)\n        )\n        filter_handles.append((sc, ESTIMATOR_LABELS.get(col, col)))\n    \n    # -------------------------------------------------------------------------\n    # Layer 2: LLM estimates (foreground, distinctive)\n    # -------------------------------------------------------------------------\n    llm_handles = []\n    for j, col in enumerate(llm_cols):\n        if col not in df_sorted.columns:\n            continue\n        y = pd.to_numeric(df_sorted[col], errors='coerce').values\n        mask = np.isfinite(y) & (y > 0)\n        x_pos = x + (j - 1) * offset_step\n        \n        style = ESTIMATOR_STYLES.get(col, {'color': PALETTE['blue'], 'marker': 'D', 'size': 36})\n        sc = ax.scatter(\n            x_pos[mask], y[mask],\n            s=style['size'], marker=style['marker'],\n            c=style['color'], alpha=0.85,\n            edgecolors='white', linewidths=0.5, zorder=2,\n            label=ESTIMATOR_LABELS.get(col, col)\n        )\n        llm_handles.append((sc, ESTIMATOR_LABELS.get(col, col)))\n    \n    # -------------------------------------------------------------------------\n    # Layer 3: Confidence intervals (top layer)\n    # -------------------------------------------------------------------------\n    central = df_sorted['_central'].values\n    lower = df_sorted['_lower'].values\n    upper = df_sorted['_upper'].values\n    use_pure = df_sorted['_use_pure_probit'].values\n    \n    # Epsilon for log scale\n    eps = 0.5\n    lower = np.maximum(lower, eps)\n    central = np.maximum(central, eps)\n    \n    # Compute error bars\n    yerr_lower = np.clip(central - lower, 0, None)\n    yerr_upper = np.clip(upper - central, 0, None)\n    \n    # Mask for valid central estimates\n    mask_valid = np.isfinite(central) & (central > 0)\n    \n    ci_handles = []\n    \n    # Pure Probit CI (forest green)\n    mask_pure = mask_valid & use_pure\n    if np.any(mask_pure):\n        err_pure = ax.errorbar(\n            x[mask_pure], central[mask_pure],\n            yerr=np.vstack([yerr_lower[mask_pure], yerr_upper[mask_pure]]),\n            fmt='o', \n            mfc='white', mec=COLORS['ci_pure_probit'], mew=1.8, ms=5,\n            ecolor=COLORS['ci_pure_probit'], elinewidth=1.2, capsize=2.5, capthick=1.2,\n            zorder=4\n        )\n        ci_handles.append((err_pure, 'Pure Probit 80% CI'))\n    \n    # Adjusted Synthetic CI (purple)\n    mask_synthetic = mask_valid & (~use_pure)\n    if np.any(mask_synthetic):\n        err_synth = ax.errorbar(\n            x[mask_synthetic], central[mask_synthetic],\n            yerr=np.vstack([yerr_lower[mask_synthetic], yerr_upper[mask_synthetic]]),\n            fmt='o', \n            mfc='white', mec=COLORS['ci_adjusted_synthetic'], mew=1.8, ms=5,\n            ecolor=COLORS['ci_adjusted_synthetic'], elinewidth=1.2, capsize=2.5, capthick=1.2,\n            zorder=4\n        )\n        ci_handles.append((err_synth, 'Adjusted Synthetic 80% CI'))\n    \n    # -------------------------------------------------------------------------\n    # Axis formatting\n    # -------------------------------------------------------------------------\n    format_log_axis(ax, axis='y', limits=(1, 10000))\n    \n    ax.set_xlabel('Organizations (sorted by ML estimate)', fontsize=FONT_SIZES['axis_label'])\n    ax.set_ylabel('Estimated ML Talent', fontsize=FONT_SIZES['axis_label'])\n    ax.set_title('ML Talent Estimates by Organization', fontsize=FONT_SIZES['title'], fontweight='medium', pad=10)\n    \n    # X-axis labels (organization names)\n    org_col = 'organization_name' if 'organization_name' in df_sorted.columns else None\n    if org_col:\n        ax.set_xticks(x)\n        ax.set_xticklabels(\n            df_sorted[org_col].astype(str).tolist(), \n            rotation=45, ha='right', \n            fontsize=FONT_SIZES['org_label']\n        )\n    \n    # -------------------------------------------------------------------------\n    # Legend \u2014 organized by category in 3 columns, positioned below plot\n    # -------------------------------------------------------------------------\n    \n    # Build legend elements for 3-column layout (interleaved for proper alignment)\n    # With ncol=3, matplotlib fills row-by-row, so we interleave:\n    # Row 1: CI header, LLM header, Keyword header\n    # Row 2: Pure Probit, Gemini, Broad Yes\n    # Row 3: Adjusted Synthetic, Claude, Strict No\n    # Row 4: (spacer), GPT-5-mini, Broad+Strict\n    \n    # Prepare CI items with correct colors from COLORS dict\n    ci_items = []\n    for handle, label in ci_handles:\n        color = COLORS['ci_pure_probit'] if 'Pure' in label else COLORS['ci_adjusted_synthetic']\n        ci_items.append(Line2D(\n            [0], [0], marker='o', color=color,\n            markerfacecolor='white', markeredgecolor=color,\n            markeredgewidth=1.8, markersize=6,\n            linestyle='-', linewidth=1.2,\n            label=f'  {label}'\n        ))\n    \n    # Prepare LLM items\n    llm_items = []\n    for handle, label in llm_handles:\n        style = [s for c, s in ESTIMATOR_STYLES.items() if ESTIMATOR_LABELS.get(c) == label]\n        if style:\n            s = style[0]\n            llm_items.append(Line2D(\n                [0], [0], marker=s['marker'], color='w',\n                markerfacecolor=s['color'], markeredgecolor='white',\n                markersize=7, linestyle='None',\n                label=f'  {label}'\n            ))\n    \n    # Prepare Keyword items\n    keyword_items = []\n    for handle, label in filter_handles:\n        style = [s for c, s in ESTIMATOR_STYLES.items() if ESTIMATOR_LABELS.get(c) == label]\n        if style:\n            s = style[0]\n            keyword_items.append(Line2D(\n                [0], [0], marker=s['marker'], color='w',\n                markerfacecolor=s['color'], markeredgecolor='none',\n                markersize=5, linestyle='None', alpha=0.5,\n                label=f'  {label}'\n            ))\n    \n    # Section headers\n    ci_header = Line2D([0], [0], color='none', label='Confidence Intervals:')\n    llm_header = Line2D([0], [0], color='none', label='LLM Estimates:')\n    keyword_header = Line2D([0], [0], color='none', label='Keyword Filters:')\n    spacer = Line2D([0], [0], color='none', linestyle='None', label=' ')  # Invisible spacer\n    \n    # Padding moved to column building below\n    \n    # Build legend as 3 columns (matplotlib ncol=3 fills column-by-column)\n    # Pad all lists to same length for even columns\n    max_len = max(len(ci_items), len(llm_items), len(keyword_items))\n    while len(ci_items) < max_len:\n        ci_items.append(spacer)\n    while len(llm_items) < max_len:\n        llm_items.append(spacer)\n    while len(keyword_items) < max_len:\n        keyword_items.append(spacer)\n    \n    # Concatenate: header + items for each column\n    legend_elements = []\n    legend_elements.append(ci_header)\n    legend_elements.extend(ci_items)\n    legend_elements.append(llm_header)\n    legend_elements.extend(llm_items)\n    legend_elements.append(keyword_header)\n    legend_elements.extend(keyword_items)\n    \n    # Position legend below plot\n    ax.legend(\n        handles=legend_elements,\n        loc='upper center',\n        bbox_to_anchor=(0.5, -0.35),\n        ncol=3,\n        fontsize=FONT_SIZES['legend'],\n        frameon=False,\n        columnspacing=2.5,\n        handletextpad=0.5,\n    )\n    \n    # Subtle grid\n    ax.grid(True, which='major', alpha=0.20, linewidth=0.4, color=COLORS['gridline'])\n    ax.set_axisbelow(True)\n    \n    # Use fixed margins only - tight_layout can cause figure size explosion\n    plt.subplots_adjust(left=0.05, right=0.95, top=0.92, bottom=0.35)\n    \n    return fig, ax, df_sorted"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create the plot for Probable organizations (q10 > 0)\n",
    "fig, ax, df_plot_sorted = create_ml_estimates_plot(df_probable, figsize=(16, 8))\n",
    "\n",
    "# Print summary\n",
    "n_pure = df_plot_sorted['_use_pure_probit'].sum()\n",
    "n_synthetic = len(df_plot_sorted) - n_pure\n",
    "print(f\"Plot Summary:\")\n",
    "print(f\"  Total organizations: {len(df_plot_sorted)}\")\n",
    "print(f\"  Using Pure Probit CI: {n_pure}\")\n",
    "print(f\"  Using Adjusted Synthetic CI: {n_synthetic}\")\n",
    "\n",
    "# Save to output directory\n",
    "if SAVE_OUTPUTS:\n",
    "    output_dir = DATA_DIR / 'output'\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    fig.savefig(output_dir / 'ml_estimates_probable.png', dpi=200)\n",
    "    print(f\"\\nSaved: {output_dir / 'ml_estimates_probable.png'}\")\n",
    "\n",
    "plt.close(fig)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 4b: ML Estimates Visualization \u2014 Confident Estimates\n\nHorizontal plot of ML estimates for organizations where the 80% CI excludes zero (q10 > 0).\n\n- **Statistical filter**: Only includes organizations where we're confident ML talent exists\n- **Rationale**: When q10 > 0, the confidence interval doesn't include zero, meaning we're uncertain about *how many* ML engineers, not *whether* there are any\n- **Axis layout**: Organizations on Y-axis, ML estimates on X-axis (for readability)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_ml_estimates_plot_all_orgs(df_plot, figsize=(16, 8), title_suffix=\"\"):\n    \"\"\"\n    Create visualization of ML estimates, filtering to confident organizations.\n    \n    NOTE: This function filters df_plot to only show organizations where q10 > 0\n    (80% CI excludes zero). The output is identical to create_ml_estimates_plot()\n    called with df_probable, but this function:\n    1. Accepts the full dataset (df_all) as input\n    2. Performs internal filtering and reports filtering statistics\n    3. Useful for documenting how many orgs were excluded\n    \n    For pre-filtered data, use create_ml_estimates_plot() instead.\n    \n    Design approach:\n    - Organizations on X-axis, ML estimates on Y-axis (standard orientation)\n    - Filters to companies where q10 > 0 (80% CI excludes zero)\n    - Keyword filters shown as small, muted gray markers (background context)\n    - LLM estimates shown as distinct colored markers (primary focus)\n    - Confidence intervals with clear visual distinction (pure probit vs synthetic)\n    \n    Args:\n        df_plot: DataFrame with ALL organization data (will be filtered internally)\n        figsize: Figure size tuple\n        title_suffix: Optional suffix for plot title\n    \n    Returns:\n        fig, ax, df_sorted (filtered to q10 > 0 only)\n    \"\"\"\n    # Estimator columns\n    filter_cols = ['filter_broad_yes', 'filter_strict_no', 'filter_broad_yes_strict_no']\n    llm_cols = ['gemini_total_accepted', 'claude_total_accepted', 'gpt5_total_accepted']\n    \n    # Make a copy and prepare data\n    df_sorted = df_plot.copy()\n    \n    # Determine which CI to use for each org\n    df_sorted['_use_pure_probit'] = pd.to_numeric(df_sorted['q50'], errors='coerce').notna()\n    \n    # Get central estimate and bounds for each org\n    df_sorted['_central'] = np.where(\n        df_sorted['_use_pure_probit'],\n        pd.to_numeric(df_sorted['q50'], errors='coerce'),\n        pd.to_numeric(df_sorted['adjusted_synthetic_q50'], errors='coerce')\n    )\n    df_sorted['_lower'] = np.where(\n        df_sorted['_use_pure_probit'],\n        pd.to_numeric(df_sorted['q10'], errors='coerce'),\n        pd.to_numeric(df_sorted['adjusted_synthetic_q10'], errors='coerce')\n    )\n    df_sorted['_upper'] = np.where(\n        df_sorted['_use_pure_probit'],\n        pd.to_numeric(df_sorted['q90'], errors='coerce'),\n        pd.to_numeric(df_sorted['adjusted_synthetic_q90'], errors='coerce')\n    )\n    \n    # Filter to companies where q10 > 0 (CI excludes zero)\n    # This is the statistically principled cut: we're confident ML talent exists\n    mask_ci_excludes_zero = df_sorted['_lower'] > 0\n    df_sorted = df_sorted[mask_ci_excludes_zero].copy()\n    \n    # Sort by central estimate\n    df_sorted['_sort_key'] = df_sorted['_central'].fillna(0)\n    df_sorted = df_sorted.sort_values('_sort_key').reset_index(drop=True)\n    \n    fig, ax = plt.subplots(figsize=figsize)\n    x = np.arange(len(df_sorted))\n    \n    # Offset for jittering points\n    offset_step = 0.10\n    \n    # -------------------------------------------------------------------------\n    # Layer 1: Keyword filter estimates (background, muted)\n    # -------------------------------------------------------------------------\n    filter_handles = []\n    for i, col in enumerate(filter_cols):\n        if col not in df_sorted.columns:\n            continue\n        y = pd.to_numeric(df_sorted[col], errors='coerce').values\n        mask = np.isfinite(y) & (y > 0)\n        x_pos = x + (i - 1) * offset_step\n        \n        style = ESTIMATOR_STYLES.get(col, {'color': PALETTE['gray'], 'marker': 'o', 'size': 24})\n        sc = ax.scatter(\n            x_pos[mask], y[mask],\n            s=style['size'], marker=style['marker'],\n            c=style['color'], alpha=0.35,\n            linewidths=0, zorder=1,\n            label=ESTIMATOR_LABELS.get(col, col)\n        )\n        filter_handles.append((sc, ESTIMATOR_LABELS.get(col, col)))\n    \n    # -------------------------------------------------------------------------\n    # Layer 2: LLM estimates (foreground, distinctive)\n    # -------------------------------------------------------------------------\n    llm_handles = []\n    for j, col in enumerate(llm_cols):\n        if col not in df_sorted.columns:\n            continue\n        y = pd.to_numeric(df_sorted[col], errors='coerce').values\n        mask = np.isfinite(y) & (y > 0)\n        x_pos = x + (j - 1) * offset_step\n        \n        style = ESTIMATOR_STYLES.get(col, {'color': PALETTE['blue'], 'marker': 'D', 'size': 36})\n        sc = ax.scatter(\n            x_pos[mask], y[mask],\n            s=style['size'], marker=style['marker'],\n            c=style['color'], alpha=0.85,\n            edgecolors='white', linewidths=0.5, zorder=2,\n            label=ESTIMATOR_LABELS.get(col, col)\n        )\n        llm_handles.append((sc, ESTIMATOR_LABELS.get(col, col)))\n    \n    # -------------------------------------------------------------------------\n    # Layer 3: Confidence intervals (top layer)\n    # -------------------------------------------------------------------------\n    central = df_sorted['_central'].values\n    lower = df_sorted['_lower'].values\n    upper = df_sorted['_upper'].values\n    use_pure = df_sorted['_use_pure_probit'].values\n    \n    # Epsilon for log scale\n    eps = 0.5\n    lower = np.maximum(lower, eps)\n    central = np.maximum(central, eps)\n    \n    # Compute error bars\n    yerr_lower = np.clip(central - lower, 0, None)\n    yerr_upper = np.clip(upper - central, 0, None)\n    \n    # Mask for valid central estimates\n    mask_valid = np.isfinite(central) & (central > 0)\n    \n    ci_handles = []\n    \n    # Pure Probit CI (forest green)\n    mask_pure = mask_valid & use_pure\n    if np.any(mask_pure):\n        err_pure = ax.errorbar(\n            x[mask_pure], central[mask_pure],\n            yerr=np.vstack([yerr_lower[mask_pure], yerr_upper[mask_pure]]),\n            fmt='o', \n            mfc='white', mec=COLORS['ci_pure_probit'], mew=1.8, ms=5,\n            ecolor=COLORS['ci_pure_probit'], elinewidth=1.2, capsize=2.5, capthick=1.2,\n            zorder=4\n        )\n        ci_handles.append((err_pure, 'Pure Probit 80% CI'))\n    \n    # Adjusted Synthetic CI (purple)\n    mask_synthetic = mask_valid & (~use_pure)\n    if np.any(mask_synthetic):\n        err_synth = ax.errorbar(\n            x[mask_synthetic], central[mask_synthetic],\n            yerr=np.vstack([yerr_lower[mask_synthetic], yerr_upper[mask_synthetic]]),\n            fmt='o', \n            mfc='white', mec=COLORS['ci_adjusted_synthetic'], mew=1.8, ms=5,\n            ecolor=COLORS['ci_adjusted_synthetic'], elinewidth=1.2, capsize=2.5, capthick=1.2,\n            zorder=4\n        )\n        ci_handles.append((err_synth, 'Adjusted Synthetic 80% CI'))\n    \n    # -------------------------------------------------------------------------\n    # Axis formatting\n    # -------------------------------------------------------------------------\n    format_log_axis(ax, axis='y', limits=(1, 10000))\n    \n    ax.set_xlabel('Organizations (sorted by ML estimate)', fontsize=FONT_SIZES['axis_label'])\n    ax.set_ylabel('Estimated ML Talent', fontsize=FONT_SIZES['axis_label'])\n    ax.set_title(f'ML Talent Estimates by Organization{title_suffix}', fontsize=FONT_SIZES['title'], fontweight='medium', pad=10)\n    \n    # X-axis labels (organization names)\n    org_col = 'organization_name' if 'organization_name' in df_sorted.columns else None\n    if org_col:\n        ax.set_xticks(x)\n        ax.set_xticklabels(\n            df_sorted[org_col].astype(str).tolist(), \n            rotation=45, ha='right', \n            fontsize=FONT_SIZES['org_label']\n        )\n    \n    # -------------------------------------------------------------------------\n    # Legend \u2014 organized by category in 3 columns, positioned below plot\n    # -------------------------------------------------------------------------\n    \n    # Build legend elements for 3-column layout (interleaved for proper alignment)\n    # With ncol=3, matplotlib fills row-by-row, so we interleave:\n    # Row 1: CI header, LLM header, Keyword header\n    # Row 2: Pure Probit, Gemini, Broad Yes\n    # Row 3: Adjusted Synthetic, Claude, Strict No\n    # Row 4: (spacer), GPT-5-mini, Broad+Strict\n    \n    # Prepare CI items with correct colors from COLORS dict\n    ci_items = []\n    for handle, label in ci_handles:\n        color = COLORS['ci_pure_probit'] if 'Pure' in label else COLORS['ci_adjusted_synthetic']\n        ci_items.append(Line2D(\n            [0], [0], marker='o', color=color,\n            markerfacecolor='white', markeredgecolor=color,\n            markeredgewidth=1.8, markersize=6,\n            linestyle='-', linewidth=1.2,\n            label=f'  {label}'\n        ))\n    \n    # Prepare LLM items\n    llm_items = []\n    for handle, label in llm_handles:\n        style = [s for c, s in ESTIMATOR_STYLES.items() if ESTIMATOR_LABELS.get(c) == label]\n        if style:\n            s = style[0]\n            llm_items.append(Line2D(\n                [0], [0], marker=s['marker'], color='w',\n                markerfacecolor=s['color'], markeredgecolor='white',\n                markersize=7, linestyle='None',\n                label=f'  {label}'\n            ))\n    \n    # Prepare Keyword items\n    keyword_items = []\n    for handle, label in filter_handles:\n        style = [s for c, s in ESTIMATOR_STYLES.items() if ESTIMATOR_LABELS.get(c) == label]\n        if style:\n            s = style[0]\n            keyword_items.append(Line2D(\n                [0], [0], marker=s['marker'], color='w',\n                markerfacecolor=s['color'], markeredgecolor='none',\n                markersize=5, linestyle='None', alpha=0.5,\n                label=f'  {label}'\n            ))\n    \n    # Section headers\n    ci_header = Line2D([0], [0], color='none', label='Confidence Intervals:')\n    llm_header = Line2D([0], [0], color='none', label='LLM Estimates:')\n    keyword_header = Line2D([0], [0], color='none', label='Keyword Filters:')\n    spacer = Line2D([0], [0], color='none', linestyle='None', label=' ')  # Invisible spacer\n    \n    # Padding moved to column building below\n    \n    # Build legend as 3 columns (matplotlib ncol=3 fills column-by-column)\n    # Pad all lists to same length for even columns\n    max_len = max(len(ci_items), len(llm_items), len(keyword_items))\n    while len(ci_items) < max_len:\n        ci_items.append(spacer)\n    while len(llm_items) < max_len:\n        llm_items.append(spacer)\n    while len(keyword_items) < max_len:\n        keyword_items.append(spacer)\n    \n    # Concatenate: header + items for each column\n    legend_elements = []\n    legend_elements.append(ci_header)\n    legend_elements.extend(ci_items)\n    legend_elements.append(llm_header)\n    legend_elements.extend(llm_items)\n    legend_elements.append(keyword_header)\n    legend_elements.extend(keyword_items)\n    \n    # Position legend below plot\n    ax.legend(\n        handles=legend_elements,\n        loc='upper center',\n        bbox_to_anchor=(0.5, -0.35),\n        ncol=3,\n        fontsize=FONT_SIZES['legend'],\n        frameon=False,\n        columnspacing=2.5,\n        handletextpad=0.5,\n    )\n    \n    # Subtle grid\n    ax.grid(True, which='major', alpha=0.20, linewidth=0.4, color=COLORS['gridline'])\n    ax.set_axisbelow(True)\n    \n    # Use fixed margins\n    plt.subplots_adjust(left=0.05, right=0.95, top=0.92, bottom=0.35)\n    \n    return fig, ax, df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create the plot for ALL organizations with confident estimates (q10 > 0)\n",
    "fig_all, ax_all, df_all_sorted = create_ml_estimates_plot_all_orgs(\n",
    "    df_all, \n",
    "    figsize=(16, 8),\n",
    "    title_suffix=\" \u2014 Confident Estimates (q10 > 0)\"\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "n_total = len(df_all)\n",
    "n_plotted = len(df_all_sorted)\n",
    "n_excluded = n_total - n_plotted\n",
    "n_pure = df_all_sorted['_use_pure_probit'].sum()\n",
    "n_synthetic = n_plotted - n_pure\n",
    "\n",
    "print(f\"Plot Summary:\")\n",
    "print(f\"  Total organizations: {n_total}\")\n",
    "print(f\"  Plotted (q10 > 0, CI excludes zero): {n_plotted}\")\n",
    "print(f\"  Excluded (q10 = 0, CI includes zero): {n_excluded}\")\n",
    "print(f\"  Using Pure Probit CI: {n_pure}\")\n",
    "print(f\"  Using Adjusted Synthetic CI: {n_synthetic}\")\n",
    "\n",
    "# Save to output directory\n",
    "if SAVE_OUTPUTS:\n",
    "    fig_all.savefig(DATA_DIR / 'output' / 'ml_estimates_all_orgs.png', dpi=200, bbox_inches='tight')\n",
    "    print(f\"\\nSaved: {DATA_DIR / 'output' / 'ml_estimates_all_orgs.png'}\")\n",
    "\n",
    "plt.close(fig_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 4c: Summary Visualizations\n\nTwo summary plots showing the distribution of ML estimate confidence across all 403 organizations:\n\n1. **Filtering Funnel**: Shows how organizations filter down from total to confident estimates\n2. **Regional Breakdown**: Stacked bar chart showing confidence levels by subregion"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Summary Visualizations: Filtering Funnel & Regional Breakdown\n",
    "# =============================================================================\n",
    "\n",
    "# Use the effective estimates already computed in Cell 17\n",
    "# df already has _q10, _q50, _q90 from mask definitions, but let's recompute for clarity\n",
    "_q10 = effective_q10\n",
    "_q50 = effective_q50\n",
    "_q90 = effective_q90\n",
    "\n",
    "# Create 4 categories using centralized function\n",
    "df['_category'] = [assign_confidence_category(q10, q50, q90) \n",
    "                   for q10, q50, q90 in zip(_q10, _q50, _q90)]\n",
    "\n",
    "# Use COLORS from Cell 1 design system for consistency\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Plot 1: Filtering Funnel (4 stages)\n",
    "# -----------------------------------------------------------------------------\n",
    "fig_funnel, ax_funnel = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "stages = ['Total\\nOrganizations', 'Non-zero\\n(q90 > 0)', 'Possible\\n(q50 > 0)', 'Probable\\n(q10 > 0)']\n",
    "values = [\n",
    "    len(df),\n",
    "    (_q90 > 0).sum(),\n",
    "    (_q50 > 0).sum(),\n",
    "    (_q10 > 0).sum()\n",
    "]\n",
    "\n",
    "colors_funnel = [COLORS['not_detected'], COLORS['nonzero'], \n",
    "                 COLORS['possible'], COLORS['probable']]\n",
    "y_pos = np.arange(len(stages))\n",
    "bars = ax_funnel.barh(y_pos, values, color=colors_funnel, edgecolor='white', linewidth=1, height=0.7)\n",
    "\n",
    "for i, (bar, val) in enumerate(zip(bars, values)):\n",
    "    pct = val / values[0] * 100\n",
    "    ax_funnel.annotate(f'{val} ({pct:.0f}%)', xy=(val + 5, bar.get_y() + bar.get_height()/2),\n",
    "                       va='center', fontsize=FONT_SIZES['annotation'], fontweight='medium')\n",
    "\n",
    "for i in range(1, len(values)):\n",
    "    drop = values[i-1] - values[i]\n",
    "    drop_pct = drop / values[i-1] * 100\n",
    "    ax_funnel.annotate(f'\u2212{drop} ({drop_pct:.0f}%)', \n",
    "                       xy=(values[i-1] - drop/2, (y_pos[i-1] + y_pos[i])/2),\n",
    "                       ha='center', va='center', fontsize=FONT_SIZES['annotation']-1, \n",
    "                       color='#DC3220', alpha=0.8)\n",
    "\n",
    "ax_funnel.set_yticks(y_pos)\n",
    "ax_funnel.set_yticklabels(stages, fontsize=FONT_SIZES['tick_label'])\n",
    "ax_funnel.set_xlabel('Number of Organizations', fontsize=FONT_SIZES['axis_label'])\n",
    "ax_funnel.set_title('Filtering Funnel: From All Organizations to Probable ML Estimates', \n",
    "                    fontsize=FONT_SIZES['title'], fontweight='medium')\n",
    "ax_funnel.set_xlim(0, max(values) * 1.25)\n",
    "ax_funnel.invert_yaxis()\n",
    "ax_funnel.spines['top'].set_visible(False)\n",
    "ax_funnel.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "if SAVE_OUTPUTS:\n",
    "    fig_funnel.savefig(DATA_DIR / 'output' / 'ml_filtering_funnel.png', dpi=200, bbox_inches='tight')\n",
    "    print(f\"Saved: {DATA_DIR / 'output' / 'ml_filtering_funnel.png'}\")\n",
    "\n",
    "plt.close(fig_funnel)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Plot 2: Stacked Bar by Region (4 categories)\n",
    "# -----------------------------------------------------------------------------\n",
    "fig_region, ax_region = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "region_col = 'Subregion' if 'Subregion' in df.columns else 'subregion'\n",
    "region_cat = pd.crosstab(df[region_col], df['_category'])\n",
    "\n",
    "col_order = ['Probable', 'Possible', 'Non-zero', 'Not Detected']\n",
    "region_cat = region_cat[[c for c in col_order if c in region_cat.columns]]\n",
    "\n",
    "region_cat['_total'] = region_cat.sum(axis=1)\n",
    "region_cat = region_cat.sort_values('_total', ascending=True).drop(columns='_total')\n",
    "\n",
    "region_cat.plot(kind='barh', stacked=True, ax=ax_region, \n",
    "                color=[COLORS['probable'], COLORS['possible'], \n",
    "                       COLORS['nonzero'], COLORS['not_detected']],\n",
    "                edgecolor='white', linewidth=0.5)\n",
    "\n",
    "ax_region.set_xlabel('Number of Organizations', fontsize=FONT_SIZES['axis_label'])\n",
    "ax_region.set_ylabel('Subregion', fontsize=FONT_SIZES['axis_label'])\n",
    "ax_region.set_title('ML Estimate Confidence by Subregion', fontsize=FONT_SIZES['title'], fontweight='medium')\n",
    "ax_region.legend(title='Category', fontsize=FONT_SIZES['legend'], frameon=False, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "if SAVE_OUTPUTS:\n",
    "    fig_region.savefig(DATA_DIR / 'output' / 'ml_confidence_by_region.png', dpi=200, bbox_inches='tight')\n",
    "    print(f\"Saved: {DATA_DIR / 'output' / 'ml_confidence_by_region.png'}\")\n",
    "\n",
    "plt.close(fig_region)\n",
    "\n",
    "print(f\"\\nCategory breakdown:\")\n",
    "print(df['_category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Step 5: ML Talent Landscape Plot\n",
    "\n",
    "Scatter plot showing ML staff count vs ML share (%) for all organizations.\n",
    "\n",
    "**Confidence Categories** (based on statistical estimates):\n",
    "- **Probable**: q10 > 0 \u2014 80% CI excludes zero\n",
    "- **Possible**: q50 > 0, q10 = 0 \u2014 Central estimate positive\n",
    "- **Non-zero**: q90 > 0, q50 = 0 \u2014 Upper bound positive only\n",
    "- **Not Detected**: All zeros"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "try:\n    from adjustText import adjust_text\n    HAS_ADJUSTTEXT = True\nexcept ImportError:\n    HAS_ADJUSTTEXT = False\n    print(\"Note: Install adjustText for better label placement: pip install adjustText\")\n\n\ndef create_landscape_plot(df_input, title_suffix=\"\", x_max=100, figsize=(11, 7), max_labels=25, log_x=True):\n    \"\"\"\n    Create ML talent landscape scatter plot.\n    \n    Design approach:\n    - Clear visual hierarchy: Probable > Possible > Non-zero > Not Detected\n    - Points colored by confidence category (based on q10/q50/q90)\n    - Labels only for \"Probable\" organizations (high confidence)\n    - Legend positioned outside plot to maximize data space\n    \n    Confidence Categories:\n    - Probable: q10 > 0 (80% CI excludes zero)\n    - Possible: q50 > 0, q10 = 0 (central estimate positive but uncertain)\n    - Non-zero: q90 > 0, q50 = 0 (upper bound positive only)\n    - Not Detected: all zeros (no ML signal)\n    \n    Args:\n        df_input: DataFrame with organization data\n        title_suffix: Optional suffix for plot title\n        x_max: Maximum x-axis value (ML share %)\n        log_x: Use logarithmic x-axis (default True)\n        figsize: Figure size tuple\n        max_labels: Maximum number of organization labels to show\n    \n    Returns:\n        fig, ax, plot_df\n    \"\"\"\n    # Prepare data - use pure probit if available, else adjusted synthetic\n    q10_pure = pd.to_numeric(df_input['q10'], errors='coerce') if 'q10' in df_input.columns else pd.Series(np.nan, index=df_input.index)\n    q50_pure = pd.to_numeric(df_input['q50'], errors='coerce') if 'q50' in df_input.columns else pd.Series(np.nan, index=df_input.index)\n    q90_pure = pd.to_numeric(df_input['q90'], errors='coerce') if 'q90' in df_input.columns else pd.Series(np.nan, index=df_input.index)\n    q10_synthetic = pd.to_numeric(df_input['adjusted_synthetic_q10'], errors='coerce')\n    q50_synthetic = pd.to_numeric(df_input['adjusted_synthetic_q50'], errors='coerce')\n    q90_synthetic = pd.to_numeric(df_input['adjusted_synthetic_q90'], errors='coerce')\n    \n    # Use pure probit when available (indicated by q50 not being NaN)\n    ml_q10 = q10_pure.where(q50_pure.notna(), q10_synthetic)\n    ml_q50 = q50_pure.where(q50_pure.notna(), q50_synthetic)\n    ml_q90 = q90_pure.where(q50_pure.notna(), q90_synthetic)\n    \n    plot_df = pd.DataFrame({\n        'org': df_input['organization_name'].astype(str),\n        'ml_n': ml_q50,  # Use q50 for plotting position\n        'ml_q10': ml_q10,\n        'ml_q90': ml_q90,\n        'emp': pd.to_numeric(df_input['total_headcount'], errors='coerce'),\n        'used_pure_probit': q50_pure.notna()\n    })\n    \n    # Calculate ML share percentage\n    plot_df['ml_pct'] = (plot_df['ml_n'] / plot_df['emp']) * 100.0\n    plot_df['ml_pct'] = plot_df['ml_pct'].clip(lower=0, upper=100)\n    \n    # Clean data\n    plot_df = plot_df.replace([np.inf, -np.inf], np.nan).dropna(subset=['ml_n', 'emp', 'ml_pct'])\n    plot_df = plot_df[(plot_df['ml_n'] >= 0) & (plot_df['emp'] > 0)]\n    \n    # Assign confidence categories\n    plot_df['cluster'] = [assign_confidence_category(q10, q50, q90) \n                          for q10, q50, q90 in plot_df[['ml_q10', 'ml_n', 'ml_q90']].values]\n    \n    # Create figure\n    fig, ax = plt.subplots(figsize=figsize)\n    \n    # -------------------------------------------------------------------------\n    # Data points by confidence category\n    # -------------------------------------------------------------------------\n    marker_sizes = {\n        'Probable': 80,\n        'Possible': 64,\n        'Non-zero': 48,\n        'Not Detected': 36,\n    }\n    \n    # Plot in order: Not Detected first (background), then others on top\n    for cluster_name in ['Not Detected', 'Non-zero', 'Possible', 'Probable']:\n        sub_df = plot_df[plot_df['cluster'] == cluster_name]\n        if len(sub_df) == 0:\n            continue\n        \n        is_highlight = cluster_name in ['Probable', 'Possible']\n        \n        ax.scatter(\n            sub_df['ml_pct'], sub_df['ml_n'],\n            s=marker_sizes[cluster_name],\n            c=LANDSCAPE_PALETTE[cluster_name],\n            marker=LANDSCAPE_MARKERS[cluster_name],\n            alpha=0.85 if is_highlight else 0.45,\n            edgecolors='white' if is_highlight else 'none',\n            linewidths=0.6 if is_highlight else 0,\n            label=cluster_name,\n            zorder=3 if is_highlight else 2\n        )\n    \n    # -------------------------------------------------------------------------\n    # Labels for Probable organizations - placed in top margin with leader lines\n    # Uses bin-packing algorithm to avoid overlaps\n    # -------------------------------------------------------------------------\n    labeled_df = plot_df[plot_df['cluster'] == 'Probable'].copy()\n    labeled_df = labeled_df.sort_values('ml_n', ascending=False).head(max_labels)\n    \n    if len(labeled_df) > 0:\n        # Sort by x-position (ml_pct) for left-to-right label placement\n        labeled_df = labeled_df.sort_values('ml_pct').reset_index(drop=True)\n        \n        # Get axis limits for positioning\n        y_min, y_max = ax.get_ylim()\n        x_min, x_max = ax.get_xlim()\n        \n        # Label positioning parameters\n        label_y_base = y_max * 0.6   # First row y position (in data coords, log scale)\n        row_multiplier = 1.8          # Each row is this much higher (log scale)\n        max_rows = 6                  # Maximum number of label rows\n        \n        # Estimate text width in log-scale data coordinates\n        def estimate_label_width_log(text, font_size=7):\n            # Width in log10 units (empirically tuned)\n            char_width = 0.08  # log10 units per character\n            return len(text) * char_width + 0.1  # Add padding\n        \n        def labels_overlap(x1, text1, x2, text2):\n            # Check if two labels would overlap (in log10 space)\n            log_x1 = np.log10(max(x1, 0.0001))\n            log_x2 = np.log10(max(x2, 0.0001))\n            \n            w1 = estimate_label_width_log(text1)\n            w2 = estimate_label_width_log(text2)\n            \n            # Check overlap: labels are centered, so half-width on each side\n            left1, right1 = log_x1 - w1/2, log_x1 + w1/2\n            left2, right2 = log_x2 - w2/2, log_x2 + w2/2\n            \n            # Overlap if intervals intersect\n            return not (right1 < left2 or right2 < left1)\n        \n        # Bin-packing: assign each label to a row\n        # Each row is a list of (x_pos, org_name, data_row)\n        rows = []\n        \n        for _, data_row in labeled_df.iterrows():\n            x_pos = data_row['ml_pct']\n            org_name = data_row['org']\n            placed = False\n            \n            # Try to place in existing row (check ALL labels in row for overlap)\n            for row_labels in rows:\n                can_place = True\n                for existing_x, existing_name, _ in row_labels:\n                    if labels_overlap(x_pos, org_name, existing_x, existing_name):\n                        can_place = False\n                        break\n                \n                if can_place:\n                    row_labels.append((x_pos, org_name, data_row))\n                    placed = True\n                    break\n            \n            # If no existing row works, create new row (up to max_rows)\n            if not placed:\n                if len(rows) < max_rows:\n                    rows.append([(x_pos, org_name, data_row)])\n                # else: skip this label (too crowded)\n        \n        # Draw labels and leader lines\n        for row_idx, row_labels in enumerate(rows):\n            label_y = label_y_base * (row_multiplier ** row_idx)\n            \n            for x_pos, org_name, data_row in row_labels:\n                point_x = data_row['ml_pct']\n                point_y = data_row['ml_n']\n                \n                # Draw leader line from point to label\n                ax.annotate(\n                    org_name,\n                    xy=(point_x, point_y),  # Point location\n                    xytext=(x_pos, label_y),  # Label location  \n                    fontsize=FONT_SIZES['org_label'],\n                    ha='center', va='bottom',\n                    color='#404040',\n                    arrowprops=dict(\n                        arrowstyle='-',\n                        lw=0.5,\n                        color='#909090',\n                        alpha=0.6,\n                        connectionstyle='arc3,rad=0'\n                    ),\n                    annotation_clip=False  # Allow drawing outside axes\n                )\n    \n    # -------------------------------------------------------------------------\n    # Axis formatting\n    # -------------------------------------------------------------------------\n    if log_x:\n        ax.set_xscale('log')\n        ax.set_xlim(0.001, x_max)  # Start at 0.001% for log scale\n        # Clean x-axis ticks for log scale\n        ax.set_xticks([0.001, 0.01, 0.1, 1, 10, 100])\n        ax.set_xticklabels(['0.001%', '0.01%', '0.1%', '1%', '10%', '100%'])\n    else:\n        ax.set_xlim(0, x_max)\n    format_log_axis(ax, axis='y', limits=(1, 50000))  # Extended to accommodate labels\n    \n    ax.set_xlabel('ML Share (%)', fontsize=FONT_SIZES['axis_label'])\n    ax.set_ylabel('ML Staff Count (q50)', fontsize=FONT_SIZES['axis_label'])\n    ax.set_title(f'ML Talent Landscape{title_suffix}', fontsize=FONT_SIZES['title'], fontweight='medium', pad=10)\n    \n    # -------------------------------------------------------------------------\n    # Legend \u2014 positioned outside, right side\n    # -------------------------------------------------------------------------\n    cluster_counts = plot_df['cluster'].value_counts()\n    \n    legend_handles = []\n    for cluster in ['Probable', 'Possible', 'Non-zero', 'Not Detected']:\n        count = cluster_counts.get(cluster, 0)\n        is_highlight = cluster in ['Probable', 'Possible']\n        legend_handles.append(\n            plt.scatter([], [], \n                s=marker_sizes[cluster] * 0.8,\n                c=LANDSCAPE_PALETTE[cluster],\n                marker=LANDSCAPE_MARKERS[cluster],\n                alpha=0.85 if is_highlight else 0.45,\n                edgecolors='white' if is_highlight else 'none',\n                linewidths=0.6 if is_highlight else 0,\n                label=f'{cluster} (n={count})'\n            )\n        )\n    \n    ax.legend(\n        handles=legend_handles,\n        loc='center left',\n        bbox_to_anchor=(1.02, 0.5),\n        frameon=True,\n        framealpha=0.95,\n        edgecolor='#E0E0E0',\n        fontsize=FONT_SIZES['legend'],\n    )\n    \n    # -------------------------------------------------------------------------\n    # Grid and styling\n    # -------------------------------------------------------------------------\n    ax.grid(True, which='major', alpha=0.20, linewidth=0.4, color=COLORS['gridline'])\n    ax.set_axisbelow(True)\n    \n    # Use fixed margins only - tight_layout can cause figure size explosion\n    plt.subplots_adjust(left=0.08, right=0.78, top=0.92, bottom=0.08)\n    \n    # Print summary of which estimate was used\n    n_pure = plot_df['used_pure_probit'].sum()\n    n_synthetic = len(plot_df) - n_pure\n    print(f\"Estimate source: {n_pure} pure probit, {n_synthetic} adjusted synthetic\")\n    \n    return fig, ax, plot_df"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot 1: All 403 organizations\n",
    "fig_all, ax_all, plot_all = create_landscape_plot(\n",
    "    df_all, \n",
    "    title_suffix=\" \u2014 All Organizations\",\n",
    "    figsize=(11, 7),\n",
    "    max_labels=25\n",
    ")\n",
    "\n",
    "print(f\"\\nCluster distribution (All Orgs, N={len(plot_all)}):\")\n",
    "for cluster in ['Probable', 'Possible', 'Non-zero', 'Not Detected']:\n",
    "    count = (plot_all['cluster'] == cluster).sum()\n",
    "    print(f\"  {cluster}: {count}\")\n",
    "\n",
    "# Save to output directory\n",
    "if SAVE_OUTPUTS:\n",
    "    output_dir = DATA_DIR / 'output'\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    fig_all.savefig(output_dir / 'ml_landscape_all.png', dpi=200)\n",
    "    print(f\"\\nSaved: {output_dir / 'ml_landscape_all.png'}\")\n",
    "\n",
    "plt.close(fig_all)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Landscape plots already saved above\nprint(\"All landscape plots saved to output directory\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Step 6: Geographic Distribution\n\nVisualize organization distribution by country and subregion:\n- **World heat map**: Country frequency choropleth\n- **Subregion bar chart**: Horizontal stacked bar chart by source category",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import plotly.express as px\nimport plotly.graph_objects as go\n\n# =============================================================================\n# PLOTLY THEME \u2014 Match Academic/Scientific Style\n# =============================================================================\n\nPLOTLY_TEMPLATE = {\n    'layout': {\n        'font': {'family': 'Helvetica Neue, Helvetica, Arial, sans-serif', 'size': 12, 'color': '#333333'},\n        'title': {'font': {'size': 14, 'color': '#333333'}, 'x': 0.5, 'xanchor': 'center'},\n        'paper_bgcolor': 'white',\n        'plot_bgcolor': 'white',\n        'colorway': PALETTE_LIST,\n        'margin': {'l': 60, 'r': 30, 't': 60, 'b': 60},\n    }\n}\n\n# Custom sequential color scale matching our palette (blue-based)\nSEQUENTIAL_COLORSCALE = [\n    [0.0, '#F7FBFF'],   # Very light blue-white\n    [0.2, '#DEEBF7'],   # Light blue\n    [0.4, '#9ECAE1'],   # Medium light blue\n    [0.6, '#4292C6'],   # Medium blue\n    [0.8, '#2171B5'],   # Darker blue\n    [1.0, '#084594'],   # Deep blue (close to our primary blue)\n]\n\n\ndef create_country_heatmap(df_input, title_suffix=\"\"):\n    \"\"\"\n    Create a world choropleth map showing country frequency.\n    \n    Design: Clean, minimal choropleth with consistent typography\n    and a sequential blue color scale.\n    \n    Args:\n        df_input: DataFrame with 'Country' column\n        title_suffix: Optional suffix for plot title\n    \n    Returns:\n        fig: Plotly figure object\n        country_counts: Series with country frequencies\n    \"\"\"\n    # Count country frequencies\n    country_counts = df_input['Country'].value_counts()\n    \n    title = f\"Geographic Distribution{title_suffix}\"\n    \n    fig = px.choropleth(\n        locations=country_counts.index.tolist(),\n        color=country_counts.values,\n        locationmode=\"country names\",\n        color_continuous_scale=SEQUENTIAL_COLORSCALE,\n        labels={\"color\": \"Organizations\"}\n    )\n    \n    fig.update_layout(\n        title={\n            'text': title,\n            'x': 0.5,\n            'xanchor': 'center',\n            'font': {'size': 14, 'family': 'Helvetica Neue, Helvetica, Arial, sans-serif', 'color': '#333333'}\n        },\n        font={'family': 'Helvetica Neue, Helvetica, Arial, sans-serif', 'size': 11, 'color': '#333333'},\n        geo=dict(\n            showframe=False,\n            showcoastlines=True,\n            coastlinecolor='#B0B0B0',\n            coastlinewidth=0.5,\n            showland=True,\n            landcolor='#F8F8F8',\n            showocean=True,\n            oceancolor='#FAFAFA',\n            showcountries=True,\n            countrycolor='#D0D0D0',\n            countrywidth=0.3,\n            projection_type='natural earth',\n        ),\n        coloraxis_colorbar=dict(\n            title=dict(text='Count', font={'size': 11}),\n            tickfont={'size': 10},\n            len=0.6,\n            thickness=15,\n            outlinewidth=0,\n        ),\n        margin={'l': 10, 'r': 10, 't': 50, 'b': 10},\n        height=420,\n        paper_bgcolor='white',\n    )\n    \n    return fig, country_counts\n\n\ndef create_subregion_bar_chart(df_input, title_suffix=\"\"):\n    \"\"\"\n    Create a horizontal bar chart showing subregion counts.\n    \n    Design: Clean horizontal bars with consistent coloring,\n    sorted by total count for easy comparison.\n    \n    Args:\n        df_input: DataFrame with 'Subregion' and optionally 'Source' columns\n        title_suffix: Optional suffix for plot title\n    \n    Returns:\n        fig: Plotly figure object\n        subregion_data: DataFrame with subregion counts\n    \"\"\"\n    # Count by subregion\n    subregion_counts = df_input['Subregion'].value_counts().sort_values(ascending=True)\n    \n    # Check if we should stack by Source\n    use_stacked = 'Source' in df_input.columns\n    \n    if use_stacked:\n        # Map Source to simplified categories\n        df_plot = df_input.copy()\n        df_plot['Source_Category'] = df_plot['Source'].apply(\n            lambda x: \"Manual Search + Network\" if x == \"Manual Search + Network\" else \"Other Sources\"\n        )\n        \n        # Cross-tab for stacked bar\n        stacked_data = pd.crosstab(df_plot['Subregion'], df_plot['Source_Category'])\n        stacked_data = stacked_data.reindex(stacked_data.sum(axis=1).sort_values(ascending=True).index)\n        \n        fig = go.Figure()\n        \n        # Use our palette colors\n        colors = {'Other Sources': COLORS['primary'], 'Manual Search + Network': COLORS['secondary']}\n        \n        for col in stacked_data.columns:\n            fig.add_trace(go.Bar(\n                name=col,\n                y=stacked_data.index.tolist(),\n                x=stacked_data[col].values,\n                orientation='h',\n                marker_color=colors.get(col, PALETTE['gray']),\n                marker_line_width=0,\n            ))\n        \n        fig.update_layout(barmode='stack')\n        subregion_data = stacked_data\n    else:\n        # Simple bar chart\n        fig = go.Figure()\n        fig.add_trace(go.Bar(\n            y=subregion_counts.index.tolist(),\n            x=subregion_counts.values,\n            orientation='h',\n            marker_color=COLORS['primary'],\n            marker_line_width=0,\n        ))\n        subregion_data = subregion_counts.to_frame(name='Count')\n    \n    title = f\"Organizations by Subregion{title_suffix}\"\n    \n    # Build layout kwargs conditionally (legend only if stacked)\n    layout_kwargs = dict(\n        title={\n            'text': title,\n            'x': 0.5,\n            'xanchor': 'center',\n            'font': {'size': 14, 'family': 'Helvetica Neue, Helvetica, Arial, sans-serif', 'color': '#333333'}\n        },\n        font={'family': 'Helvetica Neue, Helvetica, Arial, sans-serif', 'size': 11, 'color': '#333333'},\n        xaxis=dict(\n            title=dict(text='Count', font={'size': 11}),\n            tickfont={'size': 10},\n            gridcolor='#E8E8E8',\n            gridwidth=0.5,\n            zeroline=True,\n            zerolinecolor='#D0D0D0',\n            zerolinewidth=0.8,\n        ),\n        yaxis=dict(\n            title=dict(text='', font={'size': 11}),\n            tickfont={'size': 10},\n            automargin=True,\n        ),\n        margin={'l': 140, 'r': 30, 't': 70 if use_stacked else 50, 'b': 50},\n        height=max(350, len(subregion_data) * 22),\n        paper_bgcolor='white',\n        plot_bgcolor='white',\n    )\n    \n    if use_stacked:\n        layout_kwargs['legend'] = dict(\n            orientation='h',\n            yanchor='bottom',\n            y=1.02,\n            xanchor='center',\n            x=0.5,\n            font={'size': 10},\n        )\n    \n    fig.update_layout(**layout_kwargs)\n    \n    return fig, subregion_data",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create geographic visualizations for All Organizations\nfig_map_all, country_counts_all = create_country_heatmap(df_all, title_suffix=\" \u2014 All Organizations\")\nfig_map_all.show()\n\nprint(f\"\\nTop 10 countries (All Organizations, N={len(df_all)}):\")\nfor country, count in country_counts_all.head(10).items():\n    print(f\"  {country}: {count}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Subregion bar chart for All Organizations\nfig_bar_all, subregion_data_all = create_subregion_bar_chart(df_all, title_suffix=\" \u2014 All Organizations\")\nfig_bar_all.show()\n\nprint(f\"\\nSubregion totals (All Organizations):\")\nif hasattr(subregion_data_all, 'sum') and subregion_data_all.ndim > 1:\n    totals = subregion_data_all.sum(axis=1).sort_values(ascending=False)\nelse:\n    totals = subregion_data_all['Count'].sort_values(ascending=False) if 'Count' in subregion_data_all.columns else subregion_data_all.iloc[:, 0].sort_values(ascending=False)\n    \nfor region, count in totals.head(10).items():\n    print(f\"  {region}: {count}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Optional: Save geographic plots\n",
    "if SAVE_OUTPUTS:\n",
    "    output_dir = DATA_DIR / 'output'\n",
    "    \n",
    "    fig_map_all.write_html(output_dir / 'country_map_all.html')\n",
    "    fig_bar_all.write_html(output_dir / 'subregion_bar_all.html')\n",
    "    \n",
    "    print(f\"Saved geographic plots to {output_dir}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Step 7: Methodology Overview\n\nFour-panel visualization summarizing the estimation methodology:\n- **Panel A**: Estimation pipeline flowchart\n- **Panel B**: Correlation matrix showing agreement between estimators\n- **Panel C**: Beta prior distribution for prevalence\n- **Panel D**: Sensitivity vs Specificity trade-off for each estimator",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "\n",
    "# --- LOCAL DESIGN CONFIGURATION (for this figure) ---\n",
    "METHODOLOGY_COLORS = {\n",
    "    'primary': '#3C5488',       # Deep Slate Blue (Enterprise)\n",
    "    'accent_red': '#DC3220',    # Vermillion\n",
    "    'secondary': '#009988',     # Teal (Boutique/GPT)\n",
    "    'highlight': '#E68613',     # Amber (Gemini)\n",
    "    'muted_violet': '#7B4B94',  # Mid-Scale\n",
    "    'neutral': '#868686',       # Gray\n",
    "    'gridline': '#E0E0E0',\n",
    "    'text_dark': '#333333',\n",
    "    'ci_pure_probit': '#2C6E49' # Forest Green (Probit)\n",
    "}\n",
    "\n",
    "METHODOLOGY_FONT_SIZES = {\n",
    "    'title': 16,\n",
    "    'axis_label': 11,\n",
    "    'tick': 10,\n",
    "    'legend': 9\n",
    "}\n",
    "\n",
    "def setup_methodology_axis(ax):\n",
    "    \"\"\"Applies design system spine and grid rules.\"\"\"\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_linewidth(0.8)\n",
    "    ax.spines['bottom'].set_linewidth(0.8)\n",
    "    ax.spines['left'].set_color(METHODOLOGY_COLORS['text_dark'])\n",
    "    ax.spines['bottom'].set_color(METHODOLOGY_COLORS['text_dark'])\n",
    "    ax.grid(True, which='major', axis='both', color=METHODOLOGY_COLORS['gridline'], \n",
    "            linestyle='-', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "# --- DATA PREPARATION ---\n",
    "\n",
    "# Panel B: Dual Correlation Matrices (Pearson & Spearman)\n",
    "corr_labels = ['fb', 'fs', 'fbs', 'gemini', 'sonnet', 'gpt']\n",
    "\n",
    "# Pearson correlation (per-company prevalences)\n",
    "pearson_data = np.array([\n",
    "    [1.000, 0.334, 0.550, 0.326, 0.231, 0.324],\n",
    "    [0.334, 1.000, 0.231, 0.495, 0.337, 0.469],\n",
    "    [0.550, 0.231, 1.000, 0.206, 0.083, 0.165],\n",
    "    [0.326, 0.495, 0.206, 1.000, 0.818, 0.942],\n",
    "    [0.231, 0.337, 0.083, 0.818, 1.000, 0.896],\n",
    "    [0.324, 0.469, 0.165, 0.942, 0.896, 1.000]\n",
    "])\n",
    "\n",
    "# Spearman rank correlation (robust to outliers)\n",
    "spearman_data = np.array([\n",
    "    [1.000, 0.521, 0.628, 0.444, 0.440, 0.486],\n",
    "    [0.521, 1.000, 0.419, 0.544, 0.450, 0.521],\n",
    "    [0.628, 0.419, 1.000, 0.308, 0.282, 0.351],\n",
    "    [0.444, 0.544, 0.308, 1.000, 0.706, 0.853],\n",
    "    [0.440, 0.450, 0.282, 0.706, 1.000, 0.804],\n",
    "    [0.486, 0.521, 0.351, 0.853, 0.804, 1.000]\n",
    "])\n",
    "\n",
    "# Panel C: Size-dependent Beta Priors\n",
    "x_beta = np.linspace(0, 0.5, 300)\n",
    "\n",
    "size_priors = [\n",
    "    {'name': '< 100 employees', 'alpha': 2.788, 'beta': 23.087, 'color': METHODOLOGY_COLORS['secondary'], 'mean': 0.108},\n",
    "    {'name': '100 - 1K employees', 'alpha': 3.137, 'beta': 58.208, 'color': METHODOLOGY_COLORS['primary'], 'mean': 0.051},\n",
    "    {'name': '1K - 10K employees', 'alpha': 2.442, 'beta': 192.854, 'color': METHODOLOGY_COLORS['muted_violet'], 'mean': 0.013},\n",
    "    {'name': '> 10K employees', 'alpha': 1.896, 'beta': 474.957, 'color': METHODOLOGY_COLORS['highlight'], 'mean': 0.004},\n",
    "]\n",
    "\n",
    "# Panel D: Estimators (from validation data)\n",
    "methodology_estimators = [\n",
    "    {'name': 'Filter: Broad Yes', 'spec': 0.910, 'sens': 0.562, 'color': METHODOLOGY_COLORS['neutral'], 'marker': 'o', 'size': 70},\n",
    "    {'name': 'Filter: Broad+Strict', 'spec': 0.981, 'sens': 0.301, 'color': METHODOLOGY_COLORS['neutral'], 'marker': 's', 'size': 70},\n",
    "    {'name': 'Filter: Strict No', 'spec': 0.891, 'sens': 0.412, 'color': METHODOLOGY_COLORS['neutral'], 'marker': 'v', 'size': 70},\n",
    "    {'name': 'Gemini 2.5 Flash', 'spec': 0.829, 'sens': 0.712, 'color': METHODOLOGY_COLORS['highlight'], 'marker': 'P', 'size': 100},\n",
    "    {'name': 'GPT-5 Mini', 'spec': 0.868, 'sens': 0.725, 'color': METHODOLOGY_COLORS['secondary'], 'marker': '^', 'size': 100},\n",
    "    {'name': 'Claude Sonnet 4', 'spec': 0.975, 'sens': 0.588, 'color': METHODOLOGY_COLORS['primary'], 'marker': 'D', 'size': 100},\n",
    "    {'name': 'Correlated Probit', 'spec': 0.926, 'sens': 0.791, 'color': METHODOLOGY_COLORS['ci_pure_probit'], 'marker': '*', 'size': 250}\n",
    "]\n",
    "\n",
    "# --- PLOTTING ---\n",
    "\n",
    "fig = plt.figure(figsize=(18, 10), dpi=150)\n",
    "\n",
    "# Grid Configuration:\n",
    "# 2 Rows, 3 Columns (1/3 each)\n",
    "# Pipeline takes Col 0 (Rows 0-1) - 1/3 width\n",
    "# Pearson + Spearman share Cols 1-2 (Row 0) - 2/3 width total\n",
    "# Prior takes Col 1 (Row 1) - 1/3 width\n",
    "# Performance takes Col 2 (Row 1) - 1/3 width\n",
    "gs = gridspec.GridSpec(2, 3, width_ratios=[1, 1, 1], wspace=0.35, hspace=0.3)\n",
    "\n",
    "# ==========================================\n",
    "# PANEL A: ESTIMATION PIPELINE (Left Column)\n",
    "# ==========================================\n",
    "ax_pipeline = fig.add_subplot(gs[:, 0])  # Spans both rows\n",
    "ax_pipeline.set_xlim(0, 100)\n",
    "ax_pipeline.set_ylim(0, 100)\n",
    "ax_pipeline.axis('off')\n",
    "ax_pipeline.set_title(\"A. Estimation Pipeline\", loc='left', \n",
    "                      fontsize=METHODOLOGY_FONT_SIZES['title'], fontweight='medium')\n",
    "\n",
    "def draw_box(ax, x, y, w, h, text, color, subtext=\"\"):\n",
    "    rect = FancyBboxPatch((x, y), w, h, boxstyle=\"round,pad=0.5\", \n",
    "                          linewidth=1.2, edgecolor=color, facecolor='white', zorder=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x + w/2, y + h/2 + 3, text, ha='center', va='center', \n",
    "            fontsize=11, fontweight='bold', color=color, zorder=3, \n",
    "            wrap=True)\n",
    "    if subtext:\n",
    "        ax.text(x + w/2, y + h/2 - 2.5, subtext, ha='center', va='center', \n",
    "                fontsize=9, color=METHODOLOGY_COLORS['text_dark'], zorder=3,\n",
    "                wrap=True, linespacing=1.1)\n",
    "    return rect\n",
    "\n",
    "def draw_arrow(ax, x1, y1, x2, y2):\n",
    "    ax.annotate(\"\", xy=(x2, y2), xytext=(x1, y1),\n",
    "                arrowprops=dict(arrowstyle=\"->\", color=METHODOLOGY_COLORS['neutral'], lw=1.5, zorder=1))\n",
    "\n",
    "# Vertical Flow Coordinates - adjusted to use full height\n",
    "center_x = 50\n",
    "step_height = 13  # Box height\n",
    "box_width = 96    # Nearly full width\n",
    "box_x = center_x - (box_width / 2)\n",
    "gap = 4           # More padding between boxes\n",
    "start_y = 98      # Start at top\n",
    "\n",
    "# 1. Annotate\n",
    "draw_box(ax_pipeline, box_x, start_y - step_height, box_width, step_height, \n",
    "         \"1. Annotate Employees\", METHODOLOGY_COLORS['primary'], \"6 imperfect classifiers\\n(3 filters + 3 LLMs) label\\neach employee as ML or not\")\n",
    "draw_arrow(ax_pipeline, center_x, start_y - step_height, center_x, start_y - step_height - gap)\n",
    "\n",
    "# 2. Split path (Params vs Synthetic)\n",
    "split_width = 46  # Wider split boxes\n",
    "y_split = start_y - step_height - gap - step_height\n",
    "draw_box(ax_pipeline, center_x - split_width - 3, y_split, split_width, step_height, \n",
    "         \"2. Est. Accuracy\", METHODOLOGY_COLORS['secondary'], \"From ground-truth labels:\\nsens/spec per annotator\\n+ error correlations\")\n",
    "draw_box(ax_pipeline, center_x + 3, y_split, split_width, step_height, \n",
    "         \"3. Synthetic Gen\", METHODOLOGY_COLORS['muted_violet'], \"For aggregate-only firms:\\nGaussian copula generates\\nemployees w/ correlations\")\n",
    "\n",
    "# Merge arrows to Probit\n",
    "draw_arrow(ax_pipeline, center_x - split_width/2 - 3, y_split, center_x - 5, y_split - gap)\n",
    "draw_arrow(ax_pipeline, center_x + split_width/2 + 3, y_split, center_x + 5, y_split - gap)\n",
    "\n",
    "# 4. Probit\n",
    "y_probit = y_split - gap - step_height\n",
    "draw_box(ax_pipeline, box_x, y_probit, box_width, step_height, \n",
    "         \"4. Compute P(ML)\", METHODOLOGY_COLORS['ci_pure_probit'], \n",
    "         \"Probit combines 6 annotations\\nw/ correlated errors + size-\\ndependent prevalence prior\")\n",
    "draw_arrow(ax_pipeline, center_x, y_probit, center_x, y_probit - gap)\n",
    "\n",
    "# 5. Bootstrap\n",
    "y_boot = y_probit - gap - step_height\n",
    "draw_box(ax_pipeline, box_x, y_boot, box_width, step_height, \n",
    "         \"5. Bootstrap (1000x)\", METHODOLOGY_COLORS['highlight'], \n",
    "         \"Resample for uncertainty:\\nmatrices, prior, sampling,\\ncorrelations, realization\")\n",
    "draw_arrow(ax_pipeline, center_x, y_boot, center_x, y_boot - gap)\n",
    "\n",
    "# 6. Output\n",
    "y_final = y_boot - gap - step_height\n",
    "draw_box(ax_pipeline, box_x, y_final, box_width, step_height, \n",
    "         \"6. Aggregate Counts\", METHODOLOGY_COLORS['text_dark'], \"Sum P(ML) per company;\\nbootstrap gives point\\nestimates + 80% CI\")\n",
    "\n",
    "# ==========================================\n",
    "# PANEL B: DUAL CORRELATION MATRICES (Top Right, spans 2/3)\n",
    "# ==========================================\n",
    "# Custom diverging colormap (white to dark red, with light blue for negatives)\n",
    "cmap_corr = LinearSegmentedColormap.from_list('corr', ['#67a9cf', '#f7f7f7', '#8c510a', '#543005'], N=256)\n",
    "\n",
    "# Create subgridspec for the two heatmaps within cols 1-2\n",
    "gs_corr = gs[0, 1:].subgridspec(1, 2, wspace=0.4)\n",
    "\n",
    "# B1: Pearson Correlation\n",
    "ax1a = fig.add_subplot(gs_corr[0, 0])\n",
    "sns.heatmap(pearson_data, ax=ax1a, cmap=cmap_corr, annot=True, fmt=\".2f\", \n",
    "            xticklabels=corr_labels, yticklabels=corr_labels,\n",
    "            vmin=-0.4, vmax=1.0, cbar=False, annot_kws={\"size\": 9})\n",
    "ax1a.set_title(\"B. Pearson Correlation\\n(Per-Company Prevalences)\", loc='center', \n",
    "               fontsize=METHODOLOGY_FONT_SIZES['title'], fontweight='medium')\n",
    "ax1a.tick_params(axis='both', which='both', length=0)\n",
    "plt.setp(ax1a.get_xticklabels(), rotation=45, ha='right', fontsize=9)\n",
    "plt.setp(ax1a.get_yticklabels(), rotation=0, fontsize=9)\n",
    "\n",
    "# B2: Spearman Rank Correlation\n",
    "ax1b = fig.add_subplot(gs_corr[0, 1])\n",
    "sns.heatmap(spearman_data, ax=ax1b, cmap=cmap_corr, annot=True, fmt=\".2f\", \n",
    "            xticklabels=corr_labels, yticklabels=corr_labels,\n",
    "            vmin=-0.4, vmax=1.0, cbar=True, annot_kws={\"size\": 9},\n",
    "            cbar_kws={'shrink': 0.8})\n",
    "ax1b.set_title(\"Spearman Rank Correlation\\n(Robust to Outliers)\", loc='center', \n",
    "               fontsize=METHODOLOGY_FONT_SIZES['title'], fontweight='medium')\n",
    "ax1b.tick_params(axis='both', which='both', length=0)\n",
    "plt.setp(ax1b.get_xticklabels(), rotation=45, ha='right', fontsize=9)\n",
    "plt.setp(ax1b.get_yticklabels(), rotation=0, fontsize=9)\n",
    "\n",
    "# ==========================================\n",
    "# PANEL C: SIZE-DEPENDENT BETA PRIORS (Bottom Middle)\n",
    "# ==========================================\n",
    "ax2 = fig.add_subplot(gs[1, 1])\n",
    "setup_methodology_axis(ax2)\n",
    "\n",
    "# Plot each size-dependent prior\n",
    "for prior in size_priors:\n",
    "    y_beta = stats.beta.pdf(x_beta, prior['alpha'], prior['beta'])\n",
    "    ax2.plot(x_beta, y_beta, color=prior['color'], linewidth=2, \n",
    "             label=f\"{prior['name']} (\u03bc={prior['mean']:.1%})\")\n",
    "    # Add vertical line at mean\n",
    "    ax2.axvline(prior['mean'], color=prior['color'], linestyle='--', linewidth=1, alpha=0.5)\n",
    "\n",
    "ax2.set_title(\"C. Size-Dependent Prevalence Priors\", loc='left', \n",
    "              fontsize=METHODOLOGY_FONT_SIZES['title'], fontweight='medium')\n",
    "ax2.set_xlabel(r\"ML Prevalence ($\\pi$)\", fontsize=METHODOLOGY_FONT_SIZES['axis_label'])\n",
    "ax2.set_ylabel(\"Density\", fontsize=METHODOLOGY_FONT_SIZES['axis_label'])\n",
    "ax2.set_xlim(0, 0.25) \n",
    "ax2.set_ylim(0, 200)\n",
    "ax2.legend(frameon=True, fontsize=8, loc='upper right', framealpha=0.95, \n",
    "           edgecolor=METHODOLOGY_COLORS['gridline'], title='Company Size', title_fontsize=8)\n",
    "\n",
    "# ==========================================\n",
    "# PANEL D: SENSITIVITY vs SPECIFICITY (Bottom Right)\n",
    "# ==========================================\n",
    "ax3 = fig.add_subplot(gs[1, 2])\n",
    "setup_methodology_axis(ax3)\n",
    "\n",
    "# Plot points\n",
    "for est in methodology_estimators:\n",
    "    ax3.scatter(est['spec'], est['sens'], \n",
    "                color=est['color'], \n",
    "                marker=est['marker'], \n",
    "                s=est['size'], \n",
    "                edgecolor='white' if est['marker'] != '*' else 'none',\n",
    "                linewidth=0.8,\n",
    "                label=est['name'],\n",
    "                zorder=10,\n",
    "                alpha=0.9)\n",
    "\n",
    "# Styling\n",
    "ax3.set_title('D. Estimator Performance', loc='left', \n",
    "              fontsize=METHODOLOGY_FONT_SIZES['title'], fontweight='medium')\n",
    "ax3.set_xlabel('Specificity (True Negative Rate)', fontsize=METHODOLOGY_FONT_SIZES['axis_label'])\n",
    "ax3.set_ylabel('Sensitivity (True Positive Rate)', fontsize=METHODOLOGY_FONT_SIZES['axis_label'])\n",
    "ax3.set_xlim(0.0, 1.02)\n",
    "ax3.set_ylim(0.0, 1.02) \n",
    "\n",
    "# Shaded regions: highlight 0.8-1.0 bands on both axes\n",
    "# Vertical band: high specificity (0.8-1.0 on x-axis, full height)\n",
    "rect_spec = MplRectangle((0.8, 0.0), 0.22, 1.02, linewidth=0, edgecolor='none', \n",
    "                          facecolor='#3C5488', alpha=0.12)\n",
    "ax3.add_patch(rect_spec)\n",
    "# Horizontal band: high sensitivity (0.8-1.0 on y-axis, full width)\n",
    "rect_sens = MplRectangle((0.0, 0.8), 1.02, 0.22, linewidth=0, edgecolor='none', \n",
    "                          facecolor='#2C6E49', alpha=0.12)\n",
    "ax3.add_patch(rect_sens)\n",
    "\n",
    "# Custom Legend - move Probit to top\n",
    "handles, labels = ax3.get_legend_handles_labels()\n",
    "handles.insert(0, handles.pop())\n",
    "labels.insert(0, labels.pop())\n",
    "ax3.legend(handles, labels, loc='lower left', frameon=True, \n",
    "           fontsize=METHODOLOGY_FONT_SIZES['legend'], framealpha=0.95, \n",
    "           edgecolor=METHODOLOGY_COLORS['gridline'])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save to output directory\n",
    "if SAVE_OUTPUTS:\n",
    "    output_path = DATA_DIR / 'output' / 'methodology_overview.png'\n",
    "    fig.savefig(output_path, dpi=200, bbox_inches='tight', facecolor='white')\n",
    "    print(f\"Saved: {output_path}\")\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}